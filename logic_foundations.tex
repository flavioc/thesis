This chapter provides an overview of the proof theoretic basis behind LM
and the dynamic semantics of the language. First, we will present the subset of
linear logic from which LM is built on. Second, we present the high level
dynamic semantics - how rules are evaluated and node communication - followed by
the low level dynamics, a close representation of how the virtual machine runs.
Finally, we prove that the low level dynamic semantics are sound in relation to
the high level dynamic semantics.

\section{Linear Logic}

Logic, as \emph{classically} understood, treats true propositions as
\emph{persistent truth}. When a persistent proposition is needed to prove other
propositions, it can be reused as many times as we wish because it is true
indefinitely. This is also true in the \emph{constructive} or
\emph{intuitionistic} school of logic.  Linear logic is a \emph{substructural
logic} (lacks weakening and contraction) developed by
Girard~\cite{Girard95logic:its} extends \emph{persistent logic} with linear
propositions which can be understood as ephemeral resources that can be used
only once to prove other propositions.  Naturally, linear logic is well
suited for modeling computing systems that deal with state, of which LM is
one of them.  Traditional forward-chaining logic programming languages like
Datalog only use persistent logic, however many ad-hoc
extensions~\cite{Liu98extendingdatalog,Ludascher95alogical} have been devised
in to support state updates, but most are extra-logical which makes it harder
to reason about programs. LM uses linear logic as its foundation, therefore
state updates are natural to the language.

In linear logic, truth is treated as a resource that is consumed once used. For
instance, in the graph visit program in Fig.~\ref{code:visit}, the
\texttt{unvisited(A)} and \texttt{visit(A)} linear facts are consumed in order
to prove \texttt{visit(A)} and the comprehension. If those facts were
persistent, then the rule would make no sense, because the node would be
\texttt{visited} and \texttt{unvisited} at the same time!

\subsection{Sequent calculus}

We now specify and describe the fragment of the sequent calculus used for the LM
language.  In this thesis, we follow the intuitionistic approach and use the
sequent calculus~\cite{gen35} to specify logics. Our initial sequent is written
as $\Psi; \seqx{\Gamma}{\Delta}{C}$ and can be read as "assuming persistent
resources $\Psi$ and linear resources $\Delta$ then $C$ is true".  More
specifically, $\Psi$ is the typing context, $\Gamma$ is a multi-set of
persistent resources, $\Delta$ is a multi-set of linear resources while $C$ is
the proposition we want to prove.

We first have the \emph{simultaenous conjunction} $A \otimes B$ that packages
linear resources together. In the right rule, $A \otimes B$ is true if both $A$
and in the left rule, to use $A \otimes B$, it is possible to split them apart
and use them separately.

\[
\infer[\otimes R]
{\Psi ; \seqx{\Gamma}{\Delta, \Delta'}{A \otimes B}}
{\Psi ; \seqx{\Gamma}{\Delta}{A} & \seqx{\Gamma}{\Delta}{B}}
\tab
\infer[\otimes L]
{\Psi ;\seqx{\Gamma}{\Delta, A \otimes B}{C}}
{\Psi ; \seqx{\Gamma}{\Delta, A, B}{C}}
\]

To express inference, we introduce the \emph{linear implication} connective
written as $A \lolli B$. For the right rule, we prove $A \lolli B$ by assuming
$A$ and then proving $B$, while in the left rule, we obtain $B$ by using some
linear resources to prove $A$.

\[
\infer[\lolli R]
{\Psi ; \seqx{\Gamma}{\Delta}{A \lolli B}}
{\Psi ; \seqx{\Gamma}{\Delta, A}{B}}
\tab
\infer[\lolli L]
{\seqx{\Gamma}{\Delta, \Delta', A \lolli B}{C}}
{\Psi ; \seqx{\Gamma}{\Delta}{A} &
   \Psi ; \seqx{\Gamma}{\Delta', B}{C}}
\]

Next, we introduce persistent resources written as $\bang A$. For the right
rule, we prove $\bang A$ by proving it without any linear resources. Likewise,
to use a persistent resource, we simply drop the $
\bang$. There is also a $\m{copy}$ rule that moves persistent resources from
$\Gamma$ to $\Delta$. Remember that $\Gamma$ contains persistent resources.

\[
\infer[\bang R]
{\Psi ; \seqx{\Gamma}{\cdot}{\bang A}}
{\Psi ; \seqx{\Gamma}{\cdot}{A}}
\tab
\infer[\bang L]
{\Psi ; \seqx{\Gamma}{\Delta, \bang A}{C}}
{\Psi ; \seqx{\Gamma, A}{\Delta}{C}}
\tab
\infer[\m{copy}]
{\Psi ; \seqx{\Gamma, A}{\Delta}{C}}
{\Psi ; \seqx{\Gamma, A}{\Delta, A}{C}}
\]

Another useful connective is the \emph{multiplicative unit} of the $\otimes$
connective. It is written as $\one$ and is best understood as something that
does not need any resource to be proven.

\[
\infer[\one R]
{\Psi ; \seqx{\Gamma}{\cdot}{\one}}
{}
\tab
\infer[\one L]
{\Psi ; \seqx{\Gamma}{\Delta, \one}{C}}
{\Psi ; \seqx{\Gamma}{\Delta}{C}}
\]

Next, we introduce the \emph{quantification} connectives, namely \emph{universal
quantification} $\forall n:\tau. A$ and \emph{existencial quantification}
$\exists n:\tau. A$. These connectives use the typing context $\Psi$ because
they can introduce and read terms from the context. The right and left duals of
those two connectives are dual.

\[
\infer[\forall R]
{\Psi ; \seqx{\Gamma}{\Delta}{\forall n:\tau. A}}
{\Psi, m:\tau ; \seqx{\Gamma}{\Delta}{A\{m/n\}}}
\tab
\infer[\forall L]
{\Psi ; \seqx{\Gamma}{\Delta, \forall n:\tau. A}{C}}
{\Psi \vdash M : \tau & \Psi ; \seqx{\Gamma}{\Delta, A\{M/n\}}{C}}
\]

\[
\infer[\exists R]
{\Psi ; \seqx{\Gamma}{\Delta}{\exists n: \tau. A}}
{\Psi \vdash M : \tau &
   \Psi ; \seqx{\Gamma}{\Delta}{A \{M/n\}}}
\tab
\infer[\exists L]
{\Psi ; \seqx{\Gamma}{\Delta, \exists n:\tau. A}{C}}
{\Psi, m:\tau ; \seqx{\Gamma}{\Delta, A\{m/n\}}{C}}
\]

We now depart from standard linear logic and introduce \emph{iterative
definitions}. Iterative definitions are used to describe comprehensions and
aggregates. This connective is a definition that can be unfolded recursively
for an arbitrary number of times and is inspired in the Baelde's work on
least and greatest fixed points in linear
logic~\cite{Baelde:2012:LGF:2071368.2071370}. Baelde's system goes beyond
simple recursive definitions and allows proofs using induction and
co-induction in linear logic.

Iterative definitions are written as $\iters{name}{\widehat{V}}$, where $name$
is the identifier of the definition.
\[
\infer[\itersname R]
{\Psi ; \seqx{\Gamma}{\Delta}{\iters{name}{\widehat{V}}}}
{\Psi ; \seqx{\Gamma}{\Delta}{\iter{name}{N}{\widehat{V}}}}
\tab
\infer[\itersname L]
{\Psi ; \seqx{\Gamma}{\Delta, \iters{name}{\widehat{V}}}{C}}
{\Psi ; \seqx{\Gamma}{\Delta, \iter{name}{N}{\widehat{V}}}{C}}
\]

A definition $name$ has the following definition:

\begin{align}
\iter{name}{0}{\widehat{V}} & \defeq (\lambda_{\widehat{x}}. C)\widehat{V} \\
\iter{name}{N}{\widehat{V}} & \defeq \forall_{\widehat{x}}. (A \widehat{x} \lolli B
      \otimes \iter{name}{N-1}{(\iterop{x}{V})})
\end{align}

Where $\widehat{V}$ is a list of arguments and $\mathtt{op}$ is some function
that merges its arguments. Terms $A$, $B$ and $C$ are not allowed to have other
iterative definitions.

Each $\iter{name}{N}{\widehat{V}}$ has left and right rules for the case when $N
> 0$.

\[
\infer[\itersname^N R]
{\Psi ; \seqx{\Gamma}{\Delta}{\iter{name}{N}{\widehat{V}}}}
{\Psi ; \seqx{\Gamma}{\Delta}{\forall_{\widehat{x}}. (A \widehat{x} \lolli B
      \otimes \iter{name}{N-1}{(\iterop{x}{V})}})}
\]
\[
\infer[\itersname^N L]
{\Psi ; \seqx{\Gamma}{\Delta, \iter{name}{N}{\widehat{V}}}{C}}
{\Psi ; \seqx{\Gamma}{\Delta, \forall_{\widehat{x}}. (A \widehat{x} \lolli B
      \otimes \iter{name}{N-1}{(\iterop{x}{V})})}{C}}
\]

Otherwise, if $N = 0$, then the iteration stops:

\[
\infer[\itersname^0 R]
{\Psi ; \seqx{\Gamma}{\Delta}{\iter{name}{0}{\widehat{V}}}}
{\Psi ; \seqx{\Gamma}{\Delta}{(\lambda_{\widehat{x}}. A)\widehat{V}}}
\tab
\infer[\itersname^0 L]
{\Psi ; \seqx{\Gamma}{\Delta, \iter{name}{0}{\widehat{V}}}{C}}
{\Psi ; \seqx{\Gamma}{\Delta, (\lambda_{\widehat{x}}. A)\widehat{V}}{C}}
\]

Finally, we complete the linear logic system with the \emph{cut rules} and the
\emph{identity rule}:

\[
\infer[cut_A]
{\Psi ; \seqx{\Gamma}{\Delta, \Delta'}{C}}
{\Psi ; \seqx{\Gamma}{\Delta}{A} & \Psi ; \seqx{\Gamma}{\Delta', A}{C}}
\tab
\infer[cut\bang_A]
{\Psi ; \seqx{\Gamma}{\Delta}{C}}
{\Psi ; \seqx{\Gamma}{\cdot}{A} & \Psi ; \seqx{\Gamma, A}{\Delta}{C}}
\]

\[
\infer[id_A]
{\Psi ; \seqx{\Gamma}{A}{A}}
{}
\]

\subsection{Cut Reduction}

In order to use the previous linear logic system, we prove that it is
\emph{consistent}. In order to prove such thing, we first define a new sequent
$\Psi ; \seqnocut{\Gamma}{\Delta}{C}$ that includes all the previous rules
except the cut rules. Our system is consistent if we can prove anything that
uses the cut rules with only the connectives of the system.

\begin{theorem}[Cut elimination]
If $\Psi ; \seqx{\Gamma}{\Delta}{A}$ then $\Psi ; \seqnocut{\Gamma}{\Delta}{A}$.
\end{theorem}
\begin{proof}
Induction on the structure of the assumption. All cases are trivial except for
the cut rules, where we use the admissibility of cut.
\end{proof}

\begin{theorem}[Admissibility of cut]
If $\Psi ; \seqnocut{\Gamma}{\Delta}{A}$ and $\Psi ; \seqnocut{\Gamma}{\Delta',
   A}{C}$ then $\Psi ; \seqnocut{\Gamma}{\Delta, \Delta'}{C}$.
\end{theorem}
\begin{proof}
By a nested induction, first on the structure of $A$ and on the structures of
the first or the second assumption. We have to prove 3 main cases:

\begin{itemize}
   \item Identity cases: when both assumptions use the identity rule.
   \item Principal cases: when the formula $A$ is introduced by both left and
   right rules of the same connective.
   \item Commutative cases: when one of the two assumptions is fixed and the
   other is variable (depending on the rule applied). We appeal to the induction
   hypothesis because we the variable assumption gets smaller while the other
   stays the same.
\end{itemize}

We show those cases for the iterative definition only.

\paragraph{Iterative definition identity}

\[
\infer[\itersname L]
{\Psi ; \seqnocut{\Gamma}{\iters{name}{\widehat{V}}}{\iters{name}{\widehat{V}}}}
{
   \infer[\itersname R]
   {\Psi ;
      \seqnocut{\Gamma}{\iter{name}{N}{\widehat{V}}}{\iters{name}{\widehat{V}}}}
   {
      \infer[id]
      {\Psi ;
      \seqnocut{\Gamma}{\iter{name}{N}{\widehat{V}}}{\iter{name}{N}{\widehat{V}}}}
      {}
   }
}
\]

\[
\infer[\itersname^N L]
{\Psi ;
   \seqnocut{\Gamma}{\iter{name}{N}{\widehat{V}}}{\iter{name}{N}{\widehat{V}}}}
{
   \infer[\itersname^N R]
   {\Psi ; \seqnocut{\Gamma}{\iterunfold{name}{N-1}{x}{V}{A}{B}}{\iter{name}{N}{\widehat{V}}}}
   {
      \infer[id]
      {
      \Psi ; \seqnocut{\Gamma}{\iterunfold{name}{N-1}{x}{V}{A}{B}}{\iterunfold{name}{N-1}{x}{V}{A}{B}}}
      {}
   }
}
\]

\[
\infer[\itersname^0 L]
{\Psi ;
   \seqnocut{\Gamma}{\iter{name}{0}{\widehat{V}}}{\iter{name}{0}{\widehat{V}}}}
{
   \infer[\itersname^0 R]
   {
      \Psi; \seqnocut{\Gamma}{\iterunfoldz{C}{V}}{\iter{name}{0}{\widehat{V}}}
   }
   {
      \infer[id]
      {
         \Psi ; \seqnocut{\Gamma}{\iterunfoldz{C}{V}}{\iterunfoldz{C}{V}}
      }
      {}
   }
}
\]

\paragraph{Iterative definition principal case}

\begin{description}
\item[$\iters{name}{\widehat{V}}$]:
Given an inference that uses the left and right rules of the iterative
definition $\iters{name}{\widehat{V}}$:

\[
\infer[cut_{\iters{name}{\widehat{V}}}]
{
   \Psi ; \seqnocut{\Gamma}{\Delta, \Delta'}{C}
}
{
   \infer[\itersname R]
   {
      \Psi ; \seqnocut{\Gamma}{\Delta}{\iters{name}{\widehat{V}}}
   }
   {
      \Psi ; \seqnocut{\Gamma}{\Delta}{\iter{name}{N}{\widehat{V}}}
   }
   &
   \infer[\itersname L]
   {
      \Psi ; \seqnocut{\Gamma}{\Delta', \iters{name}{\widehat{V}}}{C}
   }
   {
      \Psi ; \seqnocut{\Gamma}{\Delta', \iter{name}{N}{\widehat{V}}}{C}
   }
}
\]

We can apply a cut to the smaller formulas and get the same result:

\[
\infer[cut_{\iter{name}{N}{\widehat{V}}}]
{
   \Psi ; \seqnocut{\Gamma}{\Delta, \Delta'}{C}
}
{
   \Psi ; \seqnocut{\Gamma}{\Delta}{\iter{name}{N}{\widehat{V}}}
   &
   \Psi ; \seqnocut{\Gamma}{\Delta', \iter{name}{N}{\widehat{V}}}{C}
}
\]

\item[$\iter{name}{N}{\widehat{V}}$]:
The same process can be applied to the $\iter{name}{N}{\widehat{V}}$ connective:

{\scriptsize
\[
\infer[cut_{\iter{name}{N}{\widehat{V}}}]
{
   \Psi ; \seqnocut{\Gamma}{\Delta, \Delta'}{C}
}
{
   \infer[\itersname^N R]
   {
      \Psi ; \seqnocut{\Gamma}{\Delta}{\iter{name}{N}{\widehat{V}}}
   }
   {
      \Psi ; \seqnocut{\Gamma}{\Delta}{\iterunfold{name}{N-1}{x}{V}{A}{B}}
   }
   &
   \infer[\itersname^N L]
   {
      \Psi ; \seqnocut{\Gamma}{\Delta', \iter{name}{N}{\widehat{V}}}{C}
   }
   {
      \Psi ; \seqnocut{\Gamma}{\Delta', \iterunfold{name}{N-1}{x}{V}{A}{B}}
   }
}
\]
\[
\Rightarrow
\]
\[
\infer[cut_{\iterunfold{name}{N-1}{x}{V}{A}{B}}]
{
   \Psi ; \seqnocut{\Gamma}{\Delta, \Delta'}{C}
}
{
   \Psi ; \seqnocut{\Gamma}{\Delta}{\iterunfold{name}{N-1}{x}{V}{A}{B}}
   &
   \Psi ; \seqnocut{\Gamma}{\Delta', \iterunfold{name}{N-1}{x}{V}{A}{B}}
}
\]
}

\item[$\iter{name}{0}{\widehat{V}}$]:
Finally, we only need to prove the $\iter{name}{0}{\widehat{V}}$ connective:

\[
\infer[cut_{\iter{name}{0}{\widehat{V}}}]
{
   \Psi ; \seqnocut{\Gamma}{\Delta, \Delta'}{C}
}
{
   \infer[\itersname^0 R]
   {
      \Psi ; \seqnocut{\Gamma}{\Delta}{\iter{name}{0}{\widehat{V}}}
   }
   {
      \Psi ; \seqnocut{\Gamma}{\Delta}{\iterunfoldz{A}{V}}
   }
   &
   \infer[\itersname^N L]
   {
      \Psi ; \seqnocut{\Gamma}{\Delta', \iter{name}{0}{\widehat{V}}}{C}
   }
   {
      \Psi ; \seqnocut{\Gamma}{\Delta', \iterunfoldz{A}{V}}{C}
   }
}
\]
\[
\Rightarrow
\]
\[
\infer[cut_{\iterunfoldz{A}{V}}]
{
   \Psi ; \seqnocut{\Gamma}{\Delta, \Delta'}{C}
}
{
   \Psi ; \seqnocut{\Gamma}{\Delta}{\iterunfoldz{A}{V}}
   &
   \Psi ; \seqnocut{\Gamma}{\Delta', \iterunfoldz{A}{V}}{C}
}
\]
\end{description}

\paragraph{Iterative definition commutative cases}

For the commutative cases, we use the left or right rule to one of the
assumptions and fix the other assumption and then we apply the cut using smaller
formulas.

\begin{description}
   \item[$\itersname R$]:

\[
\infer[cut_A]
{
   \Psi ; \seqnocut{\Gamma}{\Delta, \Delta'}{\iters{name}{\widehat{V}}}
}
{
   \Psi ; \seqnocut{\Gamma}{\Delta}{A}
   &
   \infer[\itersname R]
   {
      \Psi ; \seqnocut{\Gamma}{\Delta', A}{\iters{name}{\widehat{V}}}
   }
   {
      \Psi ; \seqnocut{\Gamma}{\Delta', A}{\iter{name}{N}{\widehat{V}}}
   }
}
\]
\[
\Rightarrow
\]
\[
\infer[\itersname R]
{
   \Psi ; \seqnocut{\Gamma}{\Delta, \Delta'}{\iters{name}{\widehat{V}}}
}
{
   \infer[cut_A]
   {
      \Psi ; \seqnocut{\Gamma}{\Delta, \Delta'}{\iter{name}{N}{\widehat{V}}}
   }
   {
      \Psi ; \seqnocut{\Gamma}{\Delta}{A}
      &
      \Psi ; \seqnocut{\Gamma}{\Delta', A}{\iter{name}{N}{\widehat{V}}}
   }
}
\]

   \item[$\itersname L$]:

\[
\infer[cut_A]
{
   \Psi ; \seqnocut{\Gamma}{\Delta, \Delta', \iters{name}{\widehat{V}}}{C}
}
{
   \infer[\itersname L]
   {
      \Psi; \seqnocut{\Gamma}{\Delta, \iters{name}{\widehat{V}}}{A}
   }
   {
      \Psi; \seqnocut{\Gamma}{\Delta, \iter{name}{N}{\widehat{V}}}{A}
   }
   &
   \Psi; \seqnocut{\Gamma}{\Delta', A}{C}
}
\]

\nopagebreak
\[
\Rightarrow
\]
\nopagebreak
\[
\infer[\itersname L]
{
   \Psi; \seqnocut{\Gamma}{\Delta, \Delta', \iters{name}{\widehat{V}}}{C}
}
{
   \infer[cut_A]
   {
      \Psi; \seqnocut{\Gamma}{\Delta, \Delta', \iter{name}{N}{\widehat{V}}}{C}
   }
   {
      \Psi; \seqnocut{\Gamma}{\Delta, \iter{name}{N}{\widehat{V}}}{A}
      &
      \Psi; \seqnocut{\Gamma}{\Delta', A}{C}
   }
}
\]

   \item[$\itersname^N R$]:

\[
\infer[cut_A]
{
   \Psi ; \seqnocut{\Gamma}{\Delta, \Delta'}{\iter{name}{N}{\widehat{V}}}
}
{
   \Psi ; \seqnocut{\Gamma}{\Delta}{A}
   &
   \infer[\itersname^N R]
   {
      \Psi ; \seqnocut{\Gamma}{\Delta', A}{\iter{name}{N}{\widehat{V}}}
   }
   {
      \Psi ; \seqnocut{\Gamma}{\Delta', A}{\iterunfold{name}{N-1}{x}{V}{B}{C}}
   }
}
\]
\[
\Rightarrow
\]
\[
\infer[\itersname^N R]
{
   \Psi ; \seqnocut{\Gamma}{\Delta, \Delta'}{\iter{name}{N}{\widehat{V}}}
}
{
   \infer[cut_A]
   {
      \Psi ; \seqnocut{\Gamma}{\Delta,
         \Delta'}{\iterunfold{name}{N-1}{x}{V}{B}{C}}
   }
   {
      \Psi ; \seqnocut{\Gamma}{\Delta}{A}
      &
      \Psi ; \seqnocut{\Gamma}{\Delta', A}{\iterunfold{name}{N-1}{x}{V}{B}{C}}
   }
}
\]


   \item[$\itersname^N L$]:

\[
\infer[cut_A]
{
   \Psi ; \seqnocut{\Gamma}{\Delta, \Delta', \iter{name}{N}{\widehat{V}}}{C}
}
{
   \infer[\itersname^N L]
   {
      \Psi; \seqnocut{\Gamma}{\Delta, \iter{name}{N}{\widehat{V}}}{A}
   }
   {
      \Psi; \seqnocut{\Gamma}{\Delta, \iterunfold{name}{N-1}{x}{V}{B}{D}}{A}
   }
   &
   \Psi; \seqnocut{\Gamma}{\Delta', A}{C}
}
\]
\nopagebreak
\[
\Rightarrow
\]
\nopagebreak
\[
\infer[\itersname^N L]
{
   \Psi; \seqnocut{\Gamma}{\Delta, \Delta', \iter{name}{N}{\widehat{V}}}{C}
}
{
   \infer[cut_A]
   {
      \Psi; \seqnocut{\Gamma}{\Delta, \Delta',
         \iterunfold{name}{N-1}{x}{V}{B}{D}}{C}
   }
   {
      \Psi; \seqnocut{\Gamma}{\Delta,
         \iterunfold{name}{N-1}{x}{V}{B}{D}}{A}
      &
      \Psi; \seqnocut{\Gamma}{\Delta', A}{C}
   }
}
\]

\item[$\itersname^0 R$]:

\[
\infer[cut_A]
{
   \Psi ; \seqnocut{\Gamma}{\Delta, \Delta'}{\iter{name}{0}{\widehat{V}}}
}
{
   \Psi ; \seqnocut{\Gamma}{\Delta}{A}
   &
   \infer[\itersname^0 R]
   {
      \Psi ; \seqnocut{\Gamma}{\Delta', A}{\iter{name}{0}{\widehat{V}}}
   }
   {
      \Psi ; \seqnocut{\Gamma}{\Delta', A}{\iterunfoldz{C}{V}}
   }
}
\]
\[
\Rightarrow
\]
\[
\infer[\itersname^0 R]
{
   \Psi ; \seqnocut{\Gamma}{\Delta, \Delta'}{\iter{name}{0}{\widehat{V}}}
}
{
   \infer[cut_A]
   {
      \Psi ; \seqnocut{\Gamma}{\Delta,
         \Delta'}{\iter{name}{0}{\widehat{V}}}
   }
   {
      \Psi ; \seqnocut{\Gamma}{\Delta}{A}
      &
      \Psi ; \seqnocut{\Gamma}{\Delta', A}{\iterunfoldz{C}{V}}
   }
}
\]

\item[$\itersname^0 L$]:

\[
\infer[cut_A]
{
   \Psi ; \seqnocut{\Gamma}{\Delta, \Delta', \iter{name}{0}{\widehat{V}}}{C}
}
{
   \infer[\itersname^0 L]
   {
      \Psi; \seqnocut{\Gamma}{\Delta, \iter{name}{0}{\widehat{V}}}{A}
   }
   {
      \Psi; \seqnocut{\Gamma}{\Delta, \iterunfoldz{B}{V}}{A}
   }
   &
   \Psi; \seqnocut{\Gamma}{\Delta', A}{C}
}
\]
\[
\Rightarrow
\]
\[
\infer[\itersname^0 L]
{
   \Psi ; \seqnocut{\Gamma}{\Delta, \Delta', \iter{name}{0}{\widehat{V}}}{C}
}
{
   \infer[cut_A]
   {
      \Psi; \seqnocut{\Gamma}{\Delta, \Delta', \iterunfoldz{B}{V}}{C}
   }
   {
      \Psi; \seqnocut{\Gamma}{\Delta, \iterunfoldz{B}{V}}{A}
      &
      \Psi; \seqnocut{\Gamma}{\Delta', A}{C}
   }
}
\]

\end{description}

\end{proof}

\subsection{From the sequent calculus to LM}

We summarize the connectives used in Table~\ref{table:linear}
and how they are related to the LM syntax.

\begin{table*}
\begin{center}
\resizebox{16cm}{!}{
    \begin{tabular}{ | l | l || l | l | l |}
    \hline
    Connective                   & Description                                      & LM Syntax                                  & LM Place     & LM Example                                  \\ \hline \hline
    $\emph{fact}(\hat{x})$       & Linear facts.                                    & $fact(\hat{x})$                               & Body or Head    & \texttt{path(A, P)}                            \\ \hline
    $\bang \emph{fact}(\hat{x})$ & Persistent facts.                                & $\bang fact(\hat{x})$                         & Body or Head    & \texttt{$\bang$edge(X, Y, W)}                  \\ \hline
    $1$                          & Represents rules with an empty head.             & $1$                                           & Head            & \texttt{1}                                     \\ \hline
    $A \otimes B$                & Connect two expressions.                         & $A, B$                                        & Body and Head   & \texttt{path(A, P), edge(A, B, W)}             \\ \hline
    $\forall x. A$               & To represent variables defined inside the rule.  & Please see $A \lolli B$                       & Rule            & \texttt{path(A, B) $\lolli$ reachable(A, B)}   \\ \hline
    $\exists x. A$               & Instantiates new node variables.                 & $exists \; \widehat{x}. (B)$                  & Head            & \texttt{exists A.(path(A, P))}                 \\ \hline
    $A \lolli B$                 & $\lolli$ means "linearly implies".               & $A \lolli B$                                  & Rule            & \texttt{path(A, B) $\lolli$ reachable(A, B)}   \\
                                 & $A$ is the body and $B$ is the head.             &                                               &                 &                                                \\ \hline
    $\m{def} A. B$               & Extension called definitions.                    & $\{\; \widehat{x} \; | \; A \; | \; B \; \}$  & Head            & \texttt{\{B | !edge(A, B) | visit(B)\}}        \\
                                 & Used for comprehensions and aggregates.          &                                               &                 &                                                \\ \hline
    \end{tabular}
}
\end{center}
\caption{Connectives from Linear Logic used in LM.}
\label{table:linear}
\end{table*}

Most connectives in our fragment are standard and well known, except for the $\compr{A}$ connective.

In a comprehension, we want to apply an implication as many times as possible matches we can do
using the current database. One way to formally describe comprehensions would be to use persistent
rules that would be used a few times and then would be forgotten. A more reasonable approach is to use
definitions. Given a comprehension $C = \{ \; \widehat{x} \; | \; A \; | \; B \; \}$ with a body $A$ and a head $B$, then we can build the following definition:

\[
\compr{C} \defeq 1 \with ((A \lolli B) \otimes \compr{C})
\]

We can unfold $\compr{C}$ to either stop (by selecting $1$) or get a new linear implication $A \lolli B$
to apply the comprehension once. Because we also get another $\compr{C}$ by selecting the right hand side,
the comprehension can be applied again, recursively.

Aggregates work identically, but they need an extra argument to accumulate the aggregated value. If a sum aggregate $C$ has the form $[\;\m{sum} \Rightarrow y \; | \; \widehat{x} \; | \; A \; | \; B_1 \; | \; B_2 \;]$, then the definition will be as follows:

\[
\compr{C} \; V \defeq (\lambda v. B_2)V \with (\forall x. ((Ax \lolli B_1) \otimes \compr{C} \; (x + V)))
\]

The aggregate is initiated as $\compr{C} \; 0$.

\section{High Level Dynamic Semantics}

In this section, we are going to present the high level dynamic semantics of LM. The semantics
formalize the mechanism of matching rules and deriving new facts. The high level semantics
present a simplified overview of the dynamics of the language that are closer to the formalism
of linear logic present in Fig.~\ref{linear_logic} than the implementation principles of our
virtual machine.

Both the high level and low level semantics do not model the use of variable bindings when matching
facts from the database. The formalization of bindings tends to complicate the formal system and it is not
necessary for a good understanding of the system. Instead, we assume that all facts of
type $\emph{fact}(\hat{x})$ do not have the argument $\hat{x}$.

Starting from the linear logic fragment presented earlier, we consider $\Gamma$ and $\Delta$ the database
of our program. $\Gamma$ contains the database of persistent facts while $\Delta$ the database of linear
facts. We assume that the rules of the program are persistent linear implications of the form
$\bang (A \lolli B)$ that can be used several times. However, we do not put the rules in the $\Gamma$
context but in a separate context $\Phi$.

The main idea of the dynamic semantics is to ignore the right side of the sequent calculus
and use inversion on the $\Delta$ and $\Gamma$ context so that we only have atomic facts.
To apply rules we use chaining by focusing on one of the derivation rules in $\Phi$. Note
that in the focusing process we assume that all the atoms (facts) are positive thus the chaining
becomes a forward chaining process.

\subsubsection{Application}

The main judgment of the system is $\doz \Gamma; \Delta; \Phi \rightarrow \Xi'; \Delta'; \Gamma'$.
$\Gamma$, $\Delta$ and $\Phi$ have the meaning explained before, while $\Xi'$, $\Delta'$ and $\Gamma'$
are output multi-sets from applying one of the rules in $\Phi$. $\Xi'$ is the set of consumed linear
resources, $\Delta'$ is the set of derived linear facts and $\Gamma'$ is the set of derived persistent
facts. Note that for the high level semantics there is no concept of rule priority, so we pick a rule
non-deterministically.

The judgment $\az \Gamma ; \Delta ; A \lolli B \rightarrow \Xi'; \Delta'; \Gamma'$ will attempt to apply
the derivation rule $A \lolli B$. To do this, it splits the $\Delta$ context into $\Delta_1$ and $\Delta_2$, namely the
set of linear resources consumed to match the body of the rule ($\Delta_1$) and the remaining linear facts ($\Delta_2$).
Again, the set of resources needed to match the body of the rule is guessed. The low level dynamic semantics will
deterministically determine $\Delta_1$.

\[
\infer[\az rule]
{\az \Gamma ; \Delta_1, \Delta_2 ; A \lolli B \rightarrow \Xi' ; \Delta' ; \Gamma'}
{\mz \Gamma ; \Delta_1 \rightarrow A & \dz \Gamma ; \Delta_2; \Delta_1; \cdot ; \cdot ; B \rightarrow \Xi' ; \Delta' ; \Gamma'}
\]

\[
\infer[\doz rule]
{\doz \Gamma ; \Delta ; R, \Phi \rightarrow \Xi' ; \Delta' ; \Gamma'}
{\az \Gamma ; \Delta ; R \rightarrow \Xi' ; \Delta' ; \Gamma'}
\]

\subsubsection{Match}

The $\mz \Gamma ; \Delta \rightarrow C$ judgment essentially uses the right ($R$) rules of the original
linear logic fragment in order to match all facts using $\Gamma$ and $\Delta$. We must consume all the linear facts in
the multi-set $\Delta$ when matching $C$. The context $\Gamma$ may be used to match persistent terms in $C$ but such
facts are never consumed since they are persistent.

\[
\infer[\mz 1]
{\mz \Gamma; \cdot \rightarrow 1}
{}
\tab
\infer[\mz p]
{\mz \Gamma; p \rightarrow p }
{}
\tab
\infer[\mz \bang p]
{\mz \Gamma, p; \cdot \rightarrow \bang p}
{}
\]

\[
\infer[\mz \otimes]
{\mz \Gamma; \Delta_1, \Delta_2 \rightarrow A \otimes B}
{\mz \Gamma; \Delta_1 \rightarrow A & \mz \Delta_2 \rightarrow B}
\]

\subsubsection{Derivation}

The derivation judgment has the form $\dz \Gamma ; \Delta ; \Xi ; \Gamma_1 ; \Delta_1 ; \Omega \rightarrow \Xi'; \Delta'; \Gamma'$ with the following meaning:

\begin{enumerate}
   \item[$\Gamma$] the multi-set of persistent resources in the database.
   \item[$\Delta$] the multi-set of linear resources in the database not yet consumed.
   \item[$\Xi$] the multi-set of linear resources that have been consumed while matching the body of the rule, matching comprehensions or aggregates.
   \item[$\Gamma_1$] the multi-set of persistent facts that have been derived using the current rule.
   \item[$\Delta_1$] the multi-set of linear facts that have been derived using the current rule.
   \item[$\Omega$] an ordered list contain the terms of the head of rule that still need to be derived. We start with the head of the rule $B$ that is continuously deconstructed to derive all the facts of the rule.
   \item[$\Xi'$] the consumed linear facts to apply this rule.
   \item[$\Delta'$] the derived linear facts.
   \item[$\Gamma'$] the derived persistent facts.
\end{enumerate}

We did not include the aggregates here because they are similar to comprehensions. The main rule to
derive comprehensions is $\dz comp$. It unfolds the comprehension which it can be then either
applied ($\dz \with R$ followed by $\dz \lolli$) or not ($\dz \with L$). The high level semantics
do not take into account the contents of the database to determine how many times a comprehension
should be applied because it is entirely non-deterministic.



\[
\infer[\dz p]
{\dz \Gamma ; \Delta ; \Xi ; \Gamma_1 ; \Delta_1 ; p, \Omega \rightarrow \Xi' ; \Delta' ; \Gamma'}
{\dz \Gamma ; \Delta ; \Xi ; \Gamma_1 ; p, \Delta_1 ; \Omega \rightarrow \Xi' ; \Delta' ; \Gamma'}
\]

\[
\infer[\dz \bang p]
{\dz \Gamma ; \Delta ; \Xi ; \Gamma_1 ; \Delta_1 ; \bang p, \Omega \rightarrow \Xi' ; \Delta' ; \Gamma'}
{\dz \Gamma ; \Delta ; \Xi ; \Gamma_1, p ; \Delta_1 ; \Omega \rightarrow \Xi' ; \Delta' ; \Gamma'}
\]

\[
\infer[\dz \otimes]
{\dz \Gamma ; \Delta ; \Xi ; \Gamma_1 ; \Delta_1 ; A \otimes B, \Omega \rightarrow \Xi' ; \Delta' ; \Gamma'}
{\dz \Gamma ; \Delta ; \Xi ; \Gamma_1 ; \Delta_1 ; A, B, \Omega \rightarrow \Xi' ; \Delta' ; \Gamma'}
\]

\[
\infer[\dz 1]
{\dz \Gamma ; \Delta ; \Xi ; \Gamma_1; \Delta_1 ; 1, \Omega \rightarrow \Xi' ; \Delta' ; \Gamma'}
{\dz \Gamma ; \Delta ; \Xi ; \Gamma_1; \Delta_1 ; \Omega \rightarrow \Xi' ; \Delta' ; \Gamma'}
\]

\[
\infer[\dz end]
{\dz \Gamma ; \Delta ; \Xi' ; \Gamma' ; \Delta' ; \cdot \rightarrow \Xi' ; \Delta' ; \Gamma'}
{}
\]


\[
\infer[\dz comp]
{\dz \Gamma ; \Delta ; \Xi ; \Gamma_1 ; \Delta_1 ; \comp A \lolli B, \Omega \rightarrow \Xi' ; \Delta' ; \Gamma'}
{\dz \Gamma ; \Delta ; \Xi ; \Gamma_1 ; \Delta_1 ; 1 \with (A \lolli B \otimes \comp A \lolli B), \Omega \rightarrow \Xi' ; \Delta' ; \Gamma'}
\]

\[
\infer[\dz \lolli]
{\dz \Gamma ; \Delta_a, \Delta_b ; \Xi ; \Gamma_1 ; \Delta_1 ; A \lolli B, \Omega \rightarrow \Xi' ; \Delta' ; \Gamma'}
{\mz \Gamma ; \Delta_a \rightarrow A & \dz \Gamma ; \Delta_b ; \Xi, \Delta_a ; \Gamma_1 ; \Delta_1 ; B, \Omega \rightarrow \Xi' ; \Delta' ; \Gamma'}
\]

\[
\infer[\dz \with L]
{\dz \Gamma ; \Delta ; \Xi ; \Gamma_1 ; \Delta_1 ; A \with B, \Omega \rightarrow \Xi' ; \Delta'; \Gamma'}
{\dz \Gamma ; \Delta ; \Xi ; \Gamma_1 ; \Delta_1 ; A, \Omega \rightarrow \Xi' ; \Delta'; \Gamma'}
\]

\[
\infer[\dz \with R]
{\dz \Gamma ; \Delta ; \Xi ; \Gamma_1 ; \Delta_1 ; A \with B, \Omega \rightarrow \Xi' ; \Delta' ; \Gamma'}
{\dz \Gamma ; \Delta ; \Xi ; \Gamma_1 ; \Delta_1 ; B, \Omega \rightarrow \Xi' ; \Delta' ; \Gamma'}
\]

\section{Low Level Dynamic Semantics}

The low level dynamic semantics removes all the non-deterministic choices in the previous dynamics
and makes them deterministic. The new semantics will do the following:

\begin{itemize}
   \item Match rules by priority order;
   \item Determine the set of linear facts needed to match either the body of the rule or the body of comprehensions without guessing;
   \item Apply as many comprehensions as the database allows.
\end{itemize}

The complete set of inference rules for the semantics are presented in Appendix~\ref{low_level_semantics}.

\subsection{Continuation Frames}

The new semantics introduce the concept of continuation frames. Continuation frames are choice points
created when we need to match some fact expression against the database. The frame considers all the facts
relevant to the fact expression given the current variable bindings, that may or not fail during the remaining matching process. Note that we assume
that facts have arguments and variable bindings although this is not explicit in the semantics
presented in the Appendix.
The frame contains enough state to resume the matching process at the time of its creation and
to select the next candidate fact from the database.

If at some point we cannot match a fact expression due to a wrong choice that happened earlier on,
we need to backtrack (judgment $\m{cont}$ or $\m{contc}$). Since we keep all the continuation frames in a continuation stack,
we grab the top frame and select the next fact candidate to complete the matching process.
If all candidates are exhausted, we pop the current frame and try the next one.

By using this matching mechanism, we can determine which facts need to be used to match a rule.
The virtual machine implemented for LM works pretty much in the same way, by iterating over
the available facts at each choice point and then committing to the rule if the matching process
succeeds.

\subsubsection{Continuation Frame Format}

We have two continuation frames: linear continuation frames and persistent continuation frames.

Linear frames have the form $(\Delta; \Delta_{next}; p; \Omega; \Xi; \Lambda; \Upsilon)$, where:

\begin{enumerate}
   \item[$\Delta$] Contains all the linear resources available at this choice point minus the other facts that can used to match $p$ (they are in $\Delta_{next}$).
   \item[$\Delta_{next}$] The available options to match fact $p$ if the matching fails. When backtracking, we move one fact from $\Delta_{next}$ into $\Delta$.
   \item[$p$] The fact expression that created this frame.
   \item[$\Omega$] Ordered list with the remaining terms we have to match to complete the matching process.
   \item[$\Xi$] Multi-set of consumed linear facts up to this choice point.
   \item[$\Lambda$] Multi-set of linear fact expressions already matched. All those terms must have a matching fact in $\Xi$.
   \item[$\Upsilon$] Multi-set of persistent fact expressions already matched. All those terms must have a matching fact in $\Gamma$.
\end{enumerate}

Persistent frames are simpler with the form $[\Gamma_{next}; \Delta; \bang p; \Omega; \Xi; \Lambda; \Upsilon]$, where:

\begin{enumerate}
   \item[$\Gamma_{next}$] The multi-set of available options for $\bang p$ if the current matching process fails.
   \item[$\Delta$] The multi-set of linear resources still available when the frame was created.
   \item[$\bang p$] The fact expression that created this frame.
\end{enumerate}

All the other arguments are equal to the ones used in the linear continuation frame.

\subsection{Rule Priority}

In order to respect rule priority, the low level dynamic semantics first try to match rules with
higher priority. To do this, we use a continuation frame with type $(\Phi, \Delta)$, where $\Phi$
is the list of remaining rules to try and $\Delta$ is the original $\Delta$ context. If the matching
process of the current rule fails, we use this continuation frame to try the next rule in the list.

\subsection{Comprehensions}

The matching process for both comprehensions and aggregates work slightly different than matching the
body of a rule. When the body of a rule is matched, we throw away the continuation stack since we
do not need to backtrack. For comprehensions and aggregates, every time we succeed in matching the body
we have to derive the head (for comprehensions only judgment $\m{derive}_c$) but we need to reuse the
continuation stack to
apply the comprehension (or aggregate) again. This is because the continuation stack contains,
by definition, enough information to allow the comprehension to iterate over all possible matches.

However, one needs to be careful in reusing the continuation stack. If we are matching the body
\texttt{$\bang$a(X), b(X), c(X)} and the continuation stack has three frames (one per fact), we cannot
backtrack to the frame of \texttt{c(X)} since at that point the matching process was assuming that the previous
\texttt{b(X)} linear fact was still unconsumed. Therefore we need to backtrack to the first linear
continuation frame, in this case \texttt{b(X)}. The judgment that performs such task is $\m{fix}$.
Finally, we still have to update the state of every frame
of the new continuation stack in order to remove all consumed facts from the frames ($\m{strans}$).

Note that our continuation stack is split into two stacks: $C$ and $P$. $P$ contains only
persistent continuation frames and is used first. $C$ is used for the first linear continuation frame
that appears during matching. Any persistent frame that appears afterwards is put in $C$. This makes
it easier to detect where the first linear frame is by just removing every frame except the first in $C$.

\section{Soundness Proof}

The soundness theorem proves that if a rule was successfully derived in the low level semantics
then it can also be successfully derived in the high level semantics. The completeness theorem cannot
be proven correct because the low level semantics lack the non-determinism of the high level semantics.

The first main lemma of the soundness proof proves that if we can match the body
of a rule at the low level then we can also match the rule in the high level system using the same database.

\begin{lemma}[Body Match]
   Given a match $\mo \Gamma; \Delta_1, \Delta_2; \cdot; A; B; \cdot; R \rightarrow \Xi'; \Delta'; \Gamma'$ that is related to $A$, $\Delta_1, \Delta_2$ and $\Gamma$, we get either:
   
   \begin{enumerate}
      \item $\cont \cdot; B; R; \Gamma; \Xi'; \Delta'; \Gamma'$;
      \item $\mz \Delta_2 \rightarrow A$ and $\mo \Gamma; \Delta_1; \Delta_2; \cdot; B; C'; R \rightarrow \Xi'; \Delta'; \Gamma'$ (related)
   \end{enumerate}
\end{lemma}
\begin{proof}
   Use the body match soundness theorem.
\end{proof}

When we say that a match is related to a term $A$ and a database $\Delta_1, \Delta_2, \Gamma$ we mean that
the matching judgment is related to the body $A$ of a rule and the initial database is $\Delta_1, \Delta_2, \Gamma$. Moreover, the continuation stack is related to $A$ and to the database.

The body match lemma tells us that if we start a match of a body $A$ we will either fail (1) and need to try another rule in $R$ or we succeed by building the high level matching judgment $\mz \Delta_2 \rightarrow A$ and reaching the end of the matching process $\mo \Gamma; \Delta_1; \Delta_2; \cdot; B; C'; R \rightarrow \Xi'; \Delta'; \Gamma'$.

This lemma uses a more complicated theorem that is recursively defined through judgments $\m{match}_1$ and $\m{cont}$ that use mutual induction on the size of the continuation stack, the size of the remaining terms
 to match and also the size of alternatives at each continuation frame.
 
The second stepping stone in the soundness proof is the derivation lemma. After we successfully match the
body of a rule, we need to prove that the derivation process (through judgments $\m{derive}_1$) is also
sound. This lemma is as follows:

\begin{lemma}[Derivation]
   If the low level derivation $\done \Gamma; \Delta; \Xi; \Gamma_1; \Delta_1; \Omega \rightarrow \Xi'; \Delta'; \Gamma'$ is true then the high level derivation $\dz \Gamma; \Delta; \Gamma_1; \Delta_1; \Omega \rightarrow \Xi'; \Delta'; \Gamma'$ is also true.
\end{lemma}
\begin{proof}
   Straightforward use of induction on $\Omega$ except for the sub-case of comprehensions and aggregates, where we need to use the comprehension and aggregate theorems to construct the derivation tree using $n$ applications of the corresponding construct.
\end{proof}

In the case of proving the soundness of comprehensions, we use a very identical theorem to the one used
to prove the body match soundness. However, in this case we need to reuse the continuation stack several
times (as many as many comprehensions can be applied). Using induction on the continuation stack, we get
$n$ (where $n \ge 0$) applications of the comprehension and $n \; \m{match}$ and $n \; \m{derive}$ judgments
that can be used to rebuild the derivation tree at the low level by using the $\dz \with L$, $\dz \with R$
and $\dz \lolli$ rules to fold and unfold the comprehension term. The theorem for aggregates works similarly.

\section{Summary}

In this chapter we presented the proof theoretic foundations of LM.
First, we introduced the linear logic fragment that supports LM. We then presented the
high level dynamic semantics that was created by interpreting the linear logic fragment using
focusing and chaining. Next, we designed
a formal system called the low level dynamic semantics that mimics the execution of rules in
our virtual machine minus small details.
Finally, we gave a brief overview of the soundness proof of the low level dynamic semantics,
thus proving that our virtual machine is sound in respect to our high level semantics.

