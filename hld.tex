
In this section, we present the high level dynamic~(HLD) semantics of LM.  HLD
formalizes the mechanism of matching rules and deriving new facts.  HLD
semantics present a simplified overview of the dynamics of the language that are
closer to the sequent calculus (Section~\ref{sec:fragment}) presented before
than the implementation principles of our virtual machine. The low level
dynamic~(LLD) semantics are much closer to a real implementation and
represent the operational semantics of the language. Both the HLD and LLD
semantics model local computation at the node level.

Note that both HLD and LLD do not model the use of variable bindings when
matching facts from the database. The formalization of bindings tends to
complicate the formal system and it is not necessary for a good understanding of
the system. Therefore, when we write an atomic proposition as $p$, we assume a
proposition of the form $p(\widehat{x})$, where $\widehat{x}$ is the list of
terms for that proposition (either values or variables), which is used for
matching during the derivation process.

Starting from the sequent calculus, we consider $\Gamma$ and $\Delta$ the
database of our program. $\Gamma$ contains the database of persistent facts
while $\Delta$ the database of linear facts. We assume that the rules of the
program are persistent linear implications of the form $\bang (A \lolli B)$ that
can be used several times. However, we do not put the rules in the $\Gamma$
context but in a separate context $\Phi$. The persistent terms associated with
each comprehension and aggregate are put in the $\Pi$ dictionary that maps the
unique name of the comprehension/aggregate to the persistent term.

The main idea of the dynamic semantics is to ignore the right side of the
sequent calculus and use \emph{chaining} and \emph{inversion} on the $\Delta$
and $\Gamma$ contexts so that we only have atomic facts (e.g., the database of
facts). To apply rules we use
\emph{focusing}~\cite{Andreoli92logicprogramming} on one of the derivation rules
in $\Phi$. Note that in the focusing process we assume that all the atoms
(facts) are positive thus the chaining proceeds in a \emph{forward chaining}
fashion.

\subsection{Step}\label{sec:step_hld}

Operationally, LM proceeds in \emph{steps}. A step happens at some node $i$ and
proceeds by picking one rule to apply, matching the rule's LHS against the
database, removing all those facts from the database and then deriving all the
constructs in the rule's RHS. We assume the existence of $n$ nodes in the
program and that $\Gamma$ and $\Delta$ are split into $\Gamma_1, \dotsc,
\Gamma_n$ and $\Delta_1, \dotsc, \Delta_n$ respectively. After each step, the
database of each fact is updated accordingly.

Steps are defined as $\stepz \Gamma; \Delta; \Phi \Longrightarrow \Gamma';
\Delta'$, where $\Gamma'$ and $\Delta'$ are the new database contexts and $\Phi$
are the derivation rules of the program. The step rule is as follows:

\input{hld/step}

A step is then a local derivation of some rule at a given node $i$. The effects
of a step result in the addition and retraction of facts from node $i$ and also
the assertion of facts in \emph{remote} nodes. In the rule above, this is
represented by the contexts $\Gamma'_j$ and $\Delta'_j$.

\subsection{Application}

A step is performed through
$\doz{\Gamma}{\Delta}{\Phi}{\Pi}{\Xi'}{\Gamma'}{\Delta'}$.  $\Gamma$, $\Delta$,
$\Phi$ and $\Pi$ have the meaning explained before, while $\Xi'$, $\Gamma'$ and
$\Delta'$ are output multi-sets from applying one of the rules in $\Phi$ and are
sometimes written as $\outsem$ for conciseness. $\Xi'$ is the multi-set of
retracted linear facts, $\Gamma'$ is the set of derived persistent facts and
$\Delta'$ is the multi-set of derived linear facts.  Note that for HLD semantics
there is no concept of rule priority, therefore a rule is picked
non-deterministically.

The judgment $\az{\Psi}{\Gamma}{\Delta}{\Pi}{A \lolli B}{\outsem}$ applies one
derivation rule. First, it \emph{guesses} the correct term values to the
$\forall$ variables and then splits the $\Delta$ context into $\Delta_1$ and
$\Delta_2$, namely the multi-set of linear facts consumed to match the rule's
LHS ($\Delta_1$) and the remaining linear facts ($\Delta_2$). The $\Psi$ context
is used to store a substitution that maps variables to values. In the next
section, we will see how LLD semantics deterministically calculate $\Delta_1$
and $\Psi$.

\input{hld/application}

\subsection{Match}

The $\mz{\Gamma}{\Delta}{C}$ judgment uses the right ($R$) rules of the sequent
calculus in order to match (prove) the term $C$ using $\Gamma$ and $\Delta$. We
must consume all the linear facts in the multi-set $\Delta$ when matching $C$.
The context $\Gamma$ may be used to match persistent terms in $C$ but such facts
are never retracted since they are persistent.

\input{hld/match}

\subsection{Derivation}

After successfully matching a rule's LHS, we next derive the RHS.  The
derivation judgment has the form
$\dz{\Gamma}{\Pi}{\Delta}{\Xi}{\Gamma_1}{\Delta_1}{\Omega}{\outsem}$ with the
following meaning:

\begin{enumerate}

   \item[$\Gamma$] the multi-set of persistent resources in the database;
   
   \item[$\Pi$] dictionary of persistent terms for comprehensions and
   aggregates;

   \item[$\Delta$] the multi-set of linear resources in the database not yet
   retracted;

   \item[$\Xi$] the multi-set of linear resources that have been retracted while
   matching the rule's LHS, matching comprehensions or aggregates;

   \item[$\Gamma_1$] the multi-set of persistent facts that have been derived
   using the current rule;

   \item[$\Delta_1$] the multi-set of linear facts that have been derived using
   the current rule;


   \item[$\Omega$] an ordered list which contains the propositions of the rule's RHS
      that need to asserted into the database;

   \item[$\outsem$] the output contexts, including consumed facts and derived
   persistent and linear facts.

\end{enumerate}

The following derivation rules are a direct translation from the sequent
calculus:

\input{hld/derivation}

The main rule for deriving aggregates is $\dzname \m{agg}_1$. It looks into
$\Pi$ for the appropriate persistent term and applies $\defsz{agg}$ to the
implication and then selects the recursive case. On the other hand, the rule
$\dzname \m{agg}_2$ is identical but instead decides to derive the final part of
the aggregate by selecting the additive conjunction's left hand side. The HLD
semantics do not take into account the contents of the database to determine how
many times a comprehension should be applied.

\input{hld/derivation-agg}

We do not include comprehensions here because they are a special case of
aggregates.

Finally, because both comprehensions and aggregates create implications $A \lolli
B$ and use the $\forall$ connective, we add a derivation rules $\dzname \lolli$
and $\dzname \forall$.

\input{hld/derivation-other}
