Multicore architectures have become more widespread recently and are forcing the
development of new software methodologies that enable developers to take
advantage of increasing processing power through parallelism. However, parallel
programming is difficult, usually because programs are written in imperative and
stateful programming languages that make use of low level synchronization
primitives such as locks, mutexes and barriers. This tends to make the task of
managing multi threaded execution complicated and error-prone, resulting in race
hazards and deadlocks. In the future, \emph{many-core} processors will make this
task even more daunting.

Past developments in parallel and distributed programming have given birth to
several programming models. At one end of the spectrum are the lower-level
programming abstractions such as \emph{message passing} (e.g.,
MPI~\cite{gabriel04-open-mpi}) and \emph{shared memory} (e.g.,
Pthreads~\cite{Butenhof:1997:PPT:263953} or
OpenMP~\cite{Chapman-2007-UOP-1370966}). While such abstractions give a lot of
control to the programmer and provide excellent performance, these APIs are hard
to use and debug, which makes it difficult to prove that a program is correct,
for instance. On the opposite end, we have many declarative programming
models~\cite{Blelloch:1996:PPA:227234.227246} that can exploit some form of
implicit parallelism. Implicit parallelism allows the runtime system to
automatically exploit parallelism by deciding which tasks to run in parallel.
However, it is not always obvious which tasks to run in parallel and which tasks
to run sequentially. This important issue, known as the \emph{granularity
problem}, needs to be handled well because creating too many parallel tasks will
make the program run very slowly since there is a significant overhead for
creating parallel tasks. It is then fundamental that there is a good mapping
between tasks and available processors.  Another different, but related problem,
is that declarative programming paradigms offer little to no control to the
programmer over how parallel execution is scheduled or how data is laid out,
making it hard to improve efficiency. Even if the runtime system reasonably
solves the granularity problem, there is a lack of specific information about
the program that a compiler cannot easily deduce. Furthermore, the program's
data layout is also critical for performance since poor data locality will
degrade performance even if the task granularity is optimal.  If the programmer
could provide such information, then execution would improve in terms of run
time, memory usage, or scalability.

In the context of the Claytronics project~\cite{goldstein-computer05},
Ashley-Rollman et al.~\cite{ashley-rollman-iclp09,
ashley-rollman-derosa-iros07wksp} created Meld, a logic programming language
suited to program massively distributed systems made of modular robots with a
dynamic topology. Meld programs can derive actions that are used by the
robots to act on the outside world. The distribution of computation is done by
first partitioning the program state across the robots and then by making the
computation local to the robot. Because Meld programs are sets of logical
clauses, they are more amenable to proofs than programs written using
lower-level programming abstractions.

In this thesis, we present Linear Meld (LM), a new language for parallel
programming over graph data structures that extends the original Meld
programming language with linear logic and coordination. Linear logic gives the
language a structured way to manage state, allowing the programmer to assert and
retract logical facts.  While the original Meld sees a running program as an
ensemble of robots, LM sees the program as a graph of node data structures,
where each node performs computation independently of other nodes and is able to
communicate with its neighborhood of nodes. Using the graph as the main program
abstraction, LM also solves the granularity program by allowing nodes to be
grouped into tasks that can be ran in a single thread of control.

LM introduces a new mechanism, called coordination, that is semantically
equivalent to regular computation and allows the programmer to reason about
parallel execution. Coordination introduces the concept of \emph{coordination
facts}, which are logical facts used for scheduling and data partitioning
purposes, and \emph{thread facts}, which allow the programmer to reason about
the state of the underlying parallel architecture. The use of these new
facilities moves the LM language from the paradigm of implicit parallelism to
some form of declarative explicit parallelism, but without the pitfalls of
imperative parallel programming. In turn, this makes LM a novel declarative
language that allows the programmer to optionally control how execution and data
is managed by the execution system.

Our main goal with LM is to efficiently execute provably correct declarative
graph-based programs on multi core machines. To show this, we wrote many
graph-based algorithms, proved program correctness for some programs and
developed a compiler and runtime system where we have seen good experimental
results. Finally, we have also used coordination in some programs and we were
able to see interesting improvements in terms of run time, memory usage and
scalability.

\section{Thesis Statement}

\input{statement}

