In this section, we evaluate the performance and scalability of the VM. The main
goals of this evaluation are as follows:

\begin{itemize}
   \item Compare the performance of LM programs against hand-written
      imperative C++ programs;
   \item Evaluate the scalability of said LM programs when using up to 20 cores
      concurrently;
   \item Understand the impact and effectiveness of our dynamic indexing
      algorithm and its indexing data structures used for logical facts (namely,
      hash tables);
   \item Understand the impact of the thread allocator and fact allocator on scalability and
      multicore performance;
\end{itemize}

For our experimental setup, we used a computer with a 24 (4x6) Core AMD
Opteron(tm) Processor 8425 HE $@$ 800 MHz with 64 GBytes of RAM memory running
the Linux Kernel 3.15.10-201.fc20.x86\_64. The C++ compiler used is the GCC
4.8.3 (g++) with the following \code{CXXFLAGS} flags: \code{-O3 -std=c++11
-march=x86-64}.  All experiments were executed 3 times and the running times
were averaged.

\subsection{Performance}

To understand the absolute performance of LM programs, we measured their
``sequential'' performance using a single thread of execution against
hand-written sequential C++ programs. All C++ programs were compiled with the
compilation flags that were used for LM in order to improve fairness. Arguably,
compiled C/C++ programs are a good standard for understanding the baseline
performance of new language runtimes, since compiled C/C++ programs tend to come
up on top on several popular programming language
benchmarksi~\cite{language_benchmarks}.  Furthermore, the performance of
sequential C++ programs is a better baseline for measuring the scalability of LM
since these programs are sequential and thus provide a better baseline than the
``sequential'' run time of LM programs, which includes extra code for managing
synchronization between multiple (inexistent) threads.

In order to make our comparison more interesting, we also provide comparisons to
the Python language and other relevant systems. Python is a \emph{scripting}
programming language that is much slower than compiled C/C++ programs and thus
is a good upper-bound in terms of performance.

The goal of the evaluation is to understand the effectiveness of our compilation
strategy and the effectiveness of our dynamic indexing algorithms, including the
data structures (hash tables) which are used to store index logical facts. The
LM programs used in the experiments are the following:

\begin{itemize}
   \item Multiple Single Shortest Distance (MSSD): a program that computes the
      shortest distance from a subset of nodes of the graph to the all the nodes
      in the graph. A modified version is presented in
      Section~\ref{section:coord:rationale}.

   \item N-Queens: the classic puzzle for placing queens on a chess board so
      that no two queens threaten each other. Program is explained in
      Section~\ref{section:coord:nqueens}.

   \item Belief Propagation: a machine learning to denoise images. Program is
      explained in Section~\ref{sec:coordination:bp}.

    \item Heat Transfer: an asynchronous program that performs transfer of heat
       between nodes. Program is explained in Section~\ref{section:coord:ht}.

    \item MiniMax: the AI algorithm for selecting the best player move in a
       Tic-Tac-Toe game. The initial board was augmented in order to provide a
       longer running benchmark. Program is presented in
       Section~\ref{section:coord:minimax}.
\end{itemize}

Table~\ref{table:implementation:absolute} presents the comparison between LM and
sequential C++ programs. Comparisons to other systems are shown under the
\textbf{Other} column. Since we also want to assess the VM's scalability for
different program sizes, we use different datasets in several programs.

\begin{table}[ht]
   \begin{center}
      \input{experiments/absolute/runtime}
   \end{center}
   \caption{Results.}
   \label{table:implementation:absolute}
\end{table}

For the MSSD program, we have used four datasets, which we describe as follows:

\begin{itemize}
   \item US 500 Airports~\cite{usairports,tnet}, a graph of the 500 top airports in the US with around
      5000 connections. The shortest distance is calculated for all nodes;
      
   \item OCLinks~\cite{tnet,oclinks}, a facebook-like social network with around 2000 nodes and 20000 edges. The shortest
      distance is calculated for 33\% of the nodes;

   \item US Power Grid~\cite{tnet,uspowergrid}, the power grid for western US with around 5000
      nodes and 13000 edges. The shortest distance is calculated for 20\% of the
      nodes;

   \item Email~\cite{snapnets}, a larger graph with 265000 nodes and 420000
      edges. The shortest distance was calculated for 100 nodes.

\end{itemize}

The C++'s MSSD programs applies the Dijkstra algorithm for each node we want to
compute the shortest distance from. While the Dijkstra algorithm has a better
complexity than the algorithm used in LM's algorithm, LM's algorithm is able to
process distances from multiple sources at the same time. Our experiments show
that the C++ program effectively beats LM's version by a large margin, but that
gap is reduced as the number of nodes increases. The C++ program needs to
initialize the Dijkstra algorithm for each source node, dropping its absolute
perform to just half of LM's performance. These results are also affected by the
increased locality and indexing used in LM's VM, which is shown to scale well in
our experiments. Python is much slower than LM but the gap between C++ is also
decreased as the graph gets larger.

\subsubsection{Memory usage}

\begin{table}[ht]
   \begin{center}
      \input{experiments/mem/mem}
   \end{center}
   \caption{Results.}
   \label{table:implementation:mem}
\end{table}

\begin{table}[ht]
   \begin{center}
      \input{experiments/mem/c-mem}
   \end{center}
   \caption{Results.}
   \label{table:implementation:cmem}
\end{table}

\subsubsection{Indexing evaluation}

\subsubsection{Fact allocator evaluation}

\subsection{Scalability}

\subsubsection{Thread allocator evaluation}


Say something about~\cite{cost}.
