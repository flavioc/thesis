To understand the absolute performance of LM programs, we compared their
execution time using a single thread against hand-written sequential C++
programs. All C++ programs were compiled with the same compilation flags used
for LM for fairness. Arguably, compiled C/C++ programs are a good standard for
understanding the baseline performance of new language, since compiled C/C++
programs tend to come up on top on several popular programming language
benchmarks~\cite{language_benchmarks}. Furthermore, a sequential C++ program is
also a better baseline since it does not include any synchronization or
communication code. In order to make our comparison more interesting, we also
provide comparisons against the Python language and other relevant systems.
Python is a \emph{scripting} programming language that is much slower than
compiled C/C++ programs and thus can be seen as a good upper-bound in terms of
performance.

The goal of the evaluation is to understand the effectiveness of our compilation
strategy and the effectiveness of our dynamic indexing algorithms, including the
data structures (hash tables) used to index logical facts. We used the following
programs in our experiments\footnote{All programs available on
   \url{http://github.com/flavioc/misc}.}:

\begin{itemize}
   \item Belief Propagation: a machine learning algorithm to denoise images. Program is
      presented in Section~\ref{sec:coordination:bp}.

   \item Heat Transfer: an asynchronous program that performs transfer of heat
      between nodes. Program is presented in Section~\ref{section:coord:ht}.

   \item Multiple Single Shortest Distance (MSSD): a program that computes the
      shortest distance from a subset of nodes of the graph to all the nodes in
      the graph. A modified version is later presented in
      Section~\ref{section:coord:rationale}.

    \item MiniMax: the AI algorithm for selecting the best player move in a
       Tic-Tac-Toe game. The initial board was augmented in order to provide a
       longer running benchmark. Program is presented in
       Section~\ref{section:coord:minimax}.


   \item N-Queens: the classic puzzle for placing queens on a chess board so
      that no two queens threaten each other. Program is presented in
      Section~\ref{section:coord:nqueens}.

\end{itemize}

Table~\ref{table:implementation:absolute} presents the comparison between LM and
sequential C++ programs. Comparisons to other systems are shown under the
\textbf{Other} column. Since we also want to assess the VM's scalability, we use
different dataset sizes for each program.

\begin{table}[ht]
   \begin{center}
      \input{experiments/absolute/runtime}
   \end{center}

   \mycap{Experimental results comparing different programs against
      hand-written versions in C++. For the C++ programs, we show the execution
      time in seconds (\textbf{C++ Time (s)}). For the other approaches, we show
      the overhead ratio compared with the corresponding C++ program. The
      overhead numbers (\textbf{lower is better}) are computed by dividing the
   execution time of the approach on that column by the execution time of the
similar hand-written C++ program.}

   \label{table:implementation:absolute}
\end{table}

The Belief Propagation experiment is the program where LM performs the best when
compared to the C++ version. We found out that the mathematical operations
required to update the nodes belief values are expensive and make up a huge part
of the total computation time. This is clearly shown by the low overhead
numbers. We also compared our performance against the GraphLab and LM is only
slightly slower, which is also a point in LM's favor.

The Heat Transfer program behaves somewhat like Belief Propagation but LM is
almost an order of magnitude slower than the C++ version. We think this is
because the heat transfer computation is very small which tends to exacerbate
slower and repeated fact derivations from and to different nodes in the graph.

For the MSSD program, we used seven different datasets:

\begin{itemize}
   \item US 500 Airports~\cite{usairports,tnet}, a graph of the 500 top airports in the US with around
      5000 connections. The shortest distance is calculated for all nodes;
      
   \item OCLinks~\cite{tnet,oclinks}, a Facebook-like social network with around 2000 nodes and 20000 edges. The shortest
      distance is calculated for all nodes;

   \item US Power Grid~\cite{tnet,uspowergrid}, the power grid for western US with around 5000
      nodes and 13000 edges. The shortest distance is calculated for all nodes;

   \item Twitter~\cite{snapnets,NIPS2012_4532}, a graph with 81306 nodes and 1768149 edges.
      The shortest distance is calculated for 40 nodes; 

   \item EU Email~\cite{snapnets,Leskovec:2007:GED:1217299.1217301} a graph with
      265000 nodes and 420000 edges. The shortest distance is calculated for 100
      nodes;

   \item Orkut~\cite{snapnets,Yang:2012:DEN:2350190.2350193}, a large graph
      representing data from the Orkut social network. The graph contains
      3072441 nodes and 117185083 edges.

   \item Live Journal~\cite{snapnets,Backstrom06groupformation}, a large graph representing data from the
      Live Journal social network. The graph contains 4847571 nodes and 68993773
      edges.

\end{itemize}

The C++'s MSSD version applies the Dijkstra algorithm for each node we want to
compute the shortest distance from. While the Dijkstra algorithm has a better
complexity than the algorithm used in LM, LM's algorithm is able to
process distances from multiple sources at the same time. Our experiments show
that the C++ program effectively beats LM's version by a large margin, but that
gap is reduced when using more larger graphs such as EU Email. The Python
version also uses the Dijkstra algorithm and is one order of magnitude slower
than the C++ version and usually slower than LM. We also wrote the MSSD program
in the Ligra system~\cite{Shun:2013:LLG:2517327.2442530}\footnote{Available at
   \url{http://github.com/flavioc/ligra}.}, an efficient and scalable framework
   for writing graph-based programs. Our experiments show that Ligra is, on
   average, three times as fast as our C++ program, which means that LM is
   around 15 times slower than Ligra. Note that we compiled Ligra without
   parallel support.

For the MiniMax program, we used two different starting states, Small and Big,
where the tree generated by Big is ten times larger than the one generated by
Small. The C++ version of the MiniMax program uses a single recursive function
that updates a single state as it is recursively called to generate the best
score and corresponding move. The LM version is seven to eight times slower due
to the memory requirements and costs of creating new nodes using the exists
construct. In Chapter~\ref{chapter:coordination}, we will show how to improve
the space complexity of the MiniMax program and the corresponding run time.

The LM's N-Queens programs shows some scalability issues since the overhead
ratio increases as the size of the problem increases. However, the same behavior
is seen in the Python program. The C++ version uses a backtracking strategy to
try out all the possibilities and uses a vector to store board states.  Since
there is only at most $N$ (size of the board) vectors at the same time, it shows
better behavior than all the other programs. However, we should note that a 3 to
6-fold slowdown is a fairly good trade-off for a higher-level program that will
execute faster when using more threads as we are going to observe next.

From these results, it is possible to conclude that LM's virtual machine offers
a decent performance when compared to hand-written C++ programs. We think these
results originate from four main aspects of our system: efficient indexing, good
locality with array data structures for persistent facts, an efficient memory
allocator, and a good compilation scheme. As noted in~\cite{cost}, scalability
should not be the sole focus of a parallel/distributed system such as LM. This
is even more important in declarative languages which are known to suffer from
performance issues when compared to programming languages such as C++.

\subsection{Memory usage}

To complete our comparison against the C++ programs, we have measured and
compared the average memory used by both systems.
Table~\ref{table:implementation:mem} presents the memory statistics for LM
programs while Table~\ref{table:implementation:cmem} presents statistics for the
C++ programs.

In the Belief Propagation program, LM has a much higher resource usage than the
corresponding C++ version. This is because the nodes in the LM's program keep a
copy of the belief values of neighbor nodes, while the C++ program uses shared
memory and the stack to read and compute the neighbors belief values.
Interestingly, this issue does not happen in the Heat Transfer program, where
heat values are integers and are shared between nodes, reducing the need to keep
facts with unique lists around. Notably, the C++ memory usage for Heat Transfer
is not much smaller than LM's.

\begin{table}[ht]
   \begin{center}
      \input{experiments/mem/mem}
   \end{center}

   \mycap{Memory statistics for LM programs. The meaning of each column is as
      follows: column \textbf{Average} represents the average memory use of the
      program; \textbf{Final} represents the memory usage after the program
      completes; \textbf{Malloc} represents the number of \code{malloc}
      operations requested to the operating system by the VM's memory allocator;
      \textbf{\# Facts} represents the number of facts in the database after the
      program completes; \textbf{Each} is the result of dividing \textbf{Final}
      by \textbf{\#~Facts} and represents the average memory required per fact.}

   \label{table:implementation:mem}
\end{table}

\begin{table}[ht]
   \begin{center}
      \input{experiments/mem/c-mem}
   \end{center}
   \mycap{Average and final memory usage of each C++ program.}
   \label{table:implementation:cmem}
\end{table}

When the MiniMax program completes, there are only two facts on the database
that indicate the best player move. The MiniMax program is also the only program
in this experiment that dynamically generates a (tree) graph, which is destroyed
once the best move is found. The VM's garbage collector that collects empty
nodes is able to delete the tree nodes (except the root) and the \textbf{Final}
memory usage reflects that since it is much smaller than the \textbf{Average}
statistic. However, because the garbage collector retains a small number of
freed nodes that may be reused later, the average size per fact is 15KB, which
also includes those freed nodes. Note that the memory usage of the C++ program
is much smaller because it uses function calls to represent the tree structure
of the MiniMax algorithm.

The MSSD program shows that LM's VM requires at most one order of magnitude more
memory than the corresponding C++ program. The ratio is larger when the graph
and computed distances are smaller and this is due to the extra data structures
required by the VM (i.e., the node data structure). In terms of average memory
per fact, we see that the MSSD requires on average 100B, where a big part of it
are the hash table data structures used for indexing. For the
Orkut dataset, where 100 million facts are derived, the average memory per fact
is exactly 30B, which is about the 32B required to store a particular linear
fact for representing one shortest distance.

Finally, for the N-Queens program, the results show why there are scalability
issues when using a larger $N$ since the memory usage increases significantly.
On the positive side, the average memory usage per fact remains the same for all
data sets. In respect to the C++ program, it is expected that it should consume
almost no memory because it uses the stack to compute the solutions.

\subsection{Dynamic Indexing}

We now evaluate the impact of our dynamic indexing algorithm and related data
structures. For this, we have rerun the previous experiments without indexing
and compared the results against the version with indexing enabled.
Table~\ref{table:implementation:compare_absolute} shows the comparison between
the versions with and without indexing.
Column \textbf{Run Time} shows that the MSSD program benefits from
indexing because the version without indexing is around 2 to 100 times slower than
the version with indexing enabled. Since MSSD computes the shortest distance to
multiple nodes, its rules require searching for the shortest distance facts of
arbitrary nodes. All the other programs do not require significant indexing but
also do not display any significant slowdown from using dynamic indexing. In
terms of memory usage, the version with indexing uses slightly more memory,
especially for the MSSD program, requiring, on average, 50\% more memory due to
the existence of hash tables used for supporting indexing.

\begin{table}[ht]
   \begin{center}
      \input{experiments/absolute/compare-no-indexing}
   \end{center}

   \mycap{Measuring the impact of dynamic indexing and related data
      structures. Column \textbf{Run Time} shows the slow down ratio of the
      unoptimized version (numbers greater than 1 show indexing improvements).
      Column \textbf{Average Memory} is the result of dividing the average
   memory of the optimized version by the unoptimized version (large numbers
indicate that more memory is needed when using indexing mechanisms).}

   \label{table:implementation:compare_absolute}
\end{table}

\iffalse
\subsubsection{Array data structures}

Another important implementation detail is the use of array data structures for
storing persistent facts that are not derived by rule derivation but only exist
as initial facts. These cases are detected by the compiler and allow the VM to
improve memory locality and reduce memory usage by packing persistent facts in a
contiguous memory area.

\begin{table}[ht]
   \begin{center}
      \input{experiments/absolute/compare-no-arrays}
   \end{center}

   \mycap{Measuring the impact of using array data structures for persistent
      facts. Column \textbf{Run Time} shows the speedups obtained by using
      arrays. Column \textbf{Average Memory} is the result of dividing the
      average memory of the programs using arrays by the version without them
      (e.g., numbers show memory usage reduction when using array data
   structures).}

   \label{table:implementation:compare_arrays}
\end{table}

I think we should remove this stuff XXX
\fi

