To understand the absolute performance of LM programs, we compared their
execution time using a multithreaded capable virtual machine running on a single
thread against hand-written sequential C++ programs. All C++ programs were
compiled with the same compilation flags used for LM for fairness. Arguably,
compiled C/C++ programs are a good standard for comparing the performance of new
programming languages since they tend to offer the best performance on several
popular benchmarks~\cite{language_benchmarks}.  Python is a \emph{scripting}
programming language that is much slower than compiled C/C++ programs and thus
can be seen as a good lower-bound in terms of performance.

The goal of the evaluation is to understand the effectiveness of our compilation
strategy and the effectiveness of our dynamic indexing algorithms, including the
data structures (hash tables) used to index logical facts. We used the following
programs in our experiments\footnote{All programs available on
   \url{http://github.com/flavioc/misc}.}:

\begin{itemize}
   \item Belief Propagation: a machine learning algorithm to denoise images. Program is
      presented in Section~\ref{sec:coordination:bp}.

   \item Heat Transfer: an asynchronous program that solves the heat transfer
      equation using an implicit method for transfering heat between nodes.
      Program is presented in Section~\ref{section:coord:ht}.

   \item Multiple Single Shortest Distance (MSSD): a program that computes the
      shortest distance from a subset of nodes of the graph to all the nodes in
      the graph. A modified version is later presented in
      Section~\ref{section:coord:rationale}.

    \item MiniMax: the AI algorithm for selecting the best player move in a
       Tic-Tac-Toe game. The initial board was augmented in order to provide a
       longer running benchmark. Program is presented in
       Section~\ref{section:coord:minimax}.


   \item N-Queens: the classic puzzle for placing queens on a chess board so
      that no two queens threaten each other. Program is presented in
      Section~\ref{section:coord:nqueens}.

\end{itemize}

Table~\ref{table:implementation:absolute} presents the comparison between LM and
sequential C++ programs. Comparisons to other systems are shown under the
\textbf{Other} column, which presents the speedup ratio of the C++ program over
the target system (numbers greater than 1 indicate C++ is faster). Note that the
systems used are executed with a single thread. Since we also want to assess the
VM's scalability, we use different dataset sizes for each program.

\begin{table}[ht]
   \begin{center}
      \input{experiments/absolute/runtime}
   \end{center}

   \mycap{Experimental results comparing different programs against hand-written
      versions in C++. For the C++ programs, we show the execution time in
      seconds (\textbf{C++ Time (s)}). In columns \textbf{LM} and
      \textbf{Other}, we show the speedup ratio of C++ by dividing the run time
   of the target system by the run time of the C++ program.  Numbers greater
than 1 mean that the C++ program is faster.}

   \label{table:implementation:absolute}
\end{table}

The Belief Propagation experiment is the program where LM performs the best when
compared to the C++ version. We found out that the mathematical operations
required to update the nodes belief values are expensive and make up a huge part
of the total computation time. This is clearly shown by the low overhead
numbers. We also compared our performance against the GraphLab and LM is only
slightly slower, which is also a point in LM's favor.

The Heat Transfer program behaves somewhat like Belief Propagation but LM is
almost an order of magnitude slower than the C++ version. We think this is
because the heat transfer computation is small which tends to show the overhead
of the VM.

For the MSSD program, we used seven different datasets:

\begin{itemize}
   \item US 500 Airports~\cite{usairports,tnet}, a graph of the 500 top airports in the US with around
      5000 connections. The shortest distance is calculated for all nodes;
      
   \item OCLinks~\cite{tnet,oclinks}, a Facebook-like social network with around 2000 nodes and 20000 edges. The shortest
      distance is calculated for all nodes;

   \item US Power Grid~\cite{tnet,uspowergrid}, the power grid for western US with around 5000
      nodes and 13000 edges. The shortest distance is calculated for all nodes;

   \item Twitter~\cite{snapnets,NIPS2012_4532}, a graph with 81306 nodes and 1768149 edges.
      The shortest distance is calculated for 40 nodes; 

   \item EU Email~\cite{snapnets,Leskovec:2007:GED:1217299.1217301} a graph with
      265000 nodes and 420000 edges. The shortest distance is calculated for 100
      nodes;

   \item Orkut~\cite{snapnets,Yang:2012:DEN:2350190.2350193}, a large graph
      representing data from the Orkut social network. The graph contains
      3072441 nodes and 117185083 edges.

   \item Live Journal~\cite{snapnets,Backstrom06groupformation}, a large graph representing data from the
      Live Journal social network. The graph contains 4847571 nodes and 68993773
      edges.

\end{itemize}

The C++ MSSD version applies the Dijkstra algorithm for each node we want to
compute the shortest distance from. While the Dijkstra algorithm has a better
complexity than the algorithm used in LM, LM's algorithm is able to process
distances from multiple sources at the same time. Our experiments show that the
C++ program effectively beats LM's version by a large margin, but that gap is
reduced when using larger graphs such as EU Email. The Python version also uses
the Dijkstra algorithm and is one order of magnitude slower than the C++ version
and usually slower than LM. We also wrote the MSSD program using the Ligra
system~\cite{Shun:2013:LLG:2517327.2442530}\footnote{Available at
   \url{http://github.com/flavioc/ligra}.}, an efficient and scalable framework
   for writing graph-based programs. Our experiments show that Ligra is, on
   average, three times as fast as our C++ program, which means that LM is
   around 15 times slower than Ligra. We also experimented with Valgrind's
   cachegrind tool~\cite{Nethercote:2007}, and measured the number of cache
   misses for LM, C++, and Ligra.  For instance, in the OCLinks dataset, Ligra's
   MSSD program has only a 5\% cache miss rate (for 700 million memory
   operations) while the C++ version has a 9.4\% miss rate (for 2 billion memory
   operations), which may explain the differences in performance between C++ and
   Ligra.  LM shows a 6\% cache miss rate but also a much higher number of
   memory accesses (seven times more than the C++ program).  Note that we
   compiled Ligra without parallel support.

For the MiniMax program, we used two different starting states, Small and Big,
where the tree generated by Big is ten times larger than the one generated by
Small. The C++ version of the MiniMax program uses a single recursive function
that updates a single state as it is recursively called to generate the best
score and corresponding move. The LM version is seven to eight times slower due
to the memory requirements and costs of creating new nodes using the exists
construct. In Chapter~\ref{chapter:coordination}, we will show how to improve
the space complexity of the MiniMax program and the corresponding run time.

The LM's N-Queens programs shows some scalability issues since the overhead
ratio increases as the size of the problem increases. However, the same behavior
is seen in the Python program. The C++ version uses a backtracking strategy to
try out all the possibilities and uses a vector to store board states.  Since
there is only at most $N$ (size of the board) vectors at the same time, it shows
better behavior than all the other programs. However, we should note that a 3 to
6-fold slowdown is a good trade-off for a higher-level program that will execute
faster when using more threads as we are going to observe next.

From these results, it is possible to conclude that LM's virtual machine offers
a decent performance when compared to hand-written C++ programs. We think these
results originate from four main aspects of our system: efficient indexing, good
locality with array data structures for persistent facts, an efficient memory
allocator, and a good compilation scheme. As noted in~\cite{cost}, scalability
should not be the sole focus of a parallel/distributed system such as LM. This
is even more important in declarative languages which are known to suffer from
performance issues when compared to programming languages such as C++.

\subsection{Memory Usage}

To complete our comparison against the C++ programs, we have measured and
compared the average memory used by both systems.
Table~\ref{table:implementation:mem} presents the memory statistics for LM
programs while Table~\ref{table:implementation:cmem} presents statistics of the
C++ programs and a comparison to LM's memory requirements.

In the Belief Propagation program, LM uses 2 to 3 times more memory than the
corresponding C++ version. This is because the nodes in the LM program keep a
copy of the belief values of neighbor nodes, while the C++ program uses shared
memory and the stack to read and compute the neighbors belief values. The LM
version of Heat Transfer has a slightly larger gap when compared to C++, since
heat values are represented as integers while the belief values in Belief
Propagation are represented as arrays, which is a more compact representation
when compared to the memory required to store linear facts.

\begin{table}[ht]
   \begin{center}
      \input{experiments/mem/mem}
   \end{center}

   \mycap{Memory statistics for LM programs. The meaning of each column is as
      follows: column \textbf{Average} represents the average memory use of the
      program over time; \textbf{Final} represents the memory usage after the
      program completes; \textbf{Malloc} represents the number of \code{malloc}
      operations requested to the operating system by the VM's memory allocator;
      \textbf{\# Facts} represents the number of facts in the database after the
      program completes; \textbf{Each} is the result of dividing \textbf{Final}
      by \textbf{\#~Facts} and represents the average memory required per fact.}

   \label{table:implementation:mem}
\end{table}

\begin{table}[ht]
   \begin{center}
      \input{experiments/mem/c-mem}
   \end{center}

   \mycap{Average and final memory usage of each C++ program. The columns
      \textbf{C / LM} compare the memory usage of C programs against the memory
   usage of LM programs for the average and final memory usage, respectively
(higher numbers are better).}

   \label{table:implementation:cmem}
\end{table}

When the MiniMax program completes, there are only two facts on the database
that indicate the best player move. The MiniMax program is also the only program
in this experiment that dynamically generates a (tree) graph, which is destroyed
once the best move is found. The VM's garbage collector that collects empty
nodes is able to delete the tree nodes (except the root) and the \textbf{Final}
memory usage reflects that since it is much smaller than the \textbf{Average}
statistic. However, because the garbage collector retains a small number of
freed nodes that may be reused later, the average size per fact is 15KB, which
also includes those freed nodes. Note that the memory usage of the C++ program
is much smaller because it uses function calls to represent the tree structure
of the MiniMax algorithm.

The MSSD program shows that the LM VM requires about 2 to 5 times more memory
than the corresponding C++ program. The ratio is larger when the graph and
computed distances are smaller and this is due to the extra data structures
required by the VM (i.e., the node data structure). For the Live Journal and
Orkut datasets, LM's memory usage is about the same or smaller than the C++
program. We think this is because the VM uses hash tree data structures, which
use less memory than the hash table data structures in the C++ program. In terms
of average memory per fact, we see that the MSSD requires on average 100B, where
a big part of it are the hash table data structures used for indexing. For the
Orkut dataset, where 100 million facts are derived, the average memory per fact
is exactly 30B, which is about the 32B required to store a particular linear
fact for representing one shortest distance.

For the N-Queens program, the results show why there are scalability
issues when using a larger $N$ since the memory usage increases significantly.
On the positive side, the average memory usage per fact remains the same for all
data sets. In respect to the C++ program, it is expected that it should consume
almost no memory because it uses the stack to compute the solutions.

We conclude that the LM VM has high memory requirements due to the relatively
large size of the node data structures (including the rule engine and indexing
data structures). The high memory usage tends to degrade the performance of
programs when compared to more efficient languages and systems.

\subsection{Dynamic Indexing}

In the previous experiments, we used the full functionality of the virtual
machine, including dynamic indexing and indexing data structures. We now
evaluate the impact of the dynamic indexing algorithm by running the previous
experiments without dynamic indexing.
Table~\ref{table:implementation:compare_absolute} shows the comparison between
the versions with and without indexing.  Column \textbf{Run Time} shows that the
MSSD program benefits from indexing because the version without indexing is
around 2 to 100 times slower than the version with indexing enabled. Since MSSD
computes the shortest distance to multiple nodes, its rules require searching
for the shortest distance facts of arbitrary nodes. All the other programs do
not require significant indexing but also do not display any significant
slowdown from using dynamic indexing. In terms of memory usage, the version with
indexing uses slightly more memory, especially for the MSSD program, requiring,
on average, 50\% more memory due to the existence of hash tables used to support
indexing.

\begin{table}[ht]
   \begin{center}
      \input{experiments/absolute/compare-no-indexing}
   \end{center}

   \mycap{Measuring the impact of dynamic indexing and related data structures.
      Column \textbf{Run Time} shows the slow down ratio of the unoptimized
      version (numbers greater than 1 show indexing improvements).  Column
      \textbf{Average Memory} is the result of dividing the average memory of
   the optimized version by the unoptimized version (large numbers indicate that
more memory is needed when using indexing data structures).}

   \label{table:implementation:compare_absolute}
\end{table}
