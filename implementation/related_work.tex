\paragraph{Virtual Machines}

Virtual machines are a popular technique for implementing interpreters for high
level programming languages. Due to increased availability of parallel machines
and distributed architectures, several machines have been developed with
parallelism in mind~\cite{Kara:1997:AMM:265274}. One example of such machine is
the Parallel Virtual Machine~(PVM)~\cite{Sunderam90pvm:a}, which servers as an
abstraction to program heterogeneous computers as a single machine. Another
important machine is the Threaded Abstract
Machine~(TAM)~\cite{CullerGSvE93,goldstein-tr94}, which defines a self-scheduled
machine language of parallel threads where a program is represented using
conventional control flow.

Prolog, the most prominent logic programming language, has a rich history of
virtual machine research centered around the Warren Abstract
Machine~(WAM)~\cite{AICPub641:1983}. The WAM offers special purpose
instructions, including unification instructions for different kinds of data and
control flow instructions to implement backtracking. The WAM is fully sequential
and uses four memory areas for building and matching terms: heap, stack, trail
and the push down list. Because Prolog programs tend to be naturally parallel,
much research has been done to either parallelize the WAM or create new parallel
machines. Two types of parallelism are possible in Prolog:
\emph{OR-parallelism}, where several clauses for the same goal can be executed,
and \emph{AND-parallelism}, where goals in the same clause are tried in
parallel. For OR-parallelism, we have several models such as: the SRI
model~\cite{Warren:1987:OEM:67683.67699}, the Argonne
model~\cite{ButlerDLOOS88}, the MUSE model~\cite{Ali:1990fk} and the BC
machine~\cite{Ali88}. For AND-parallelism, different implementations were built
on top of the WAM~\cite{Hermenegildo:1986:AMB:913061,Lin:1988:AEL:900478},
however more general models such as the Andorra
model~\cite{Haridi:1990:KAP:87961.87964} were developed which allows both AND
and OR-parallelism.

\paragraph{Datalog Execution} The Datalog language is a forwards-chaining logic
programming and therefore requires different evaluation strategies than those
used by Prolog, which is a backwards-chaining logic programming language.
Arguably, the most well-known strategy for Datalog programs with recursive rules
is the \emph{Semi-Na\"{\i}ve Fixpoint}
algorithm~\cite{Balbin:1987:GDA:34657.34661}, where the computation is split
into iterations and the facts generated in the previous generation are used as
inputs to derive the facts in the next iteration. The advantage of this
mechanism is that no redundant computations are performed, which could happen in
the case of recursive rules but is avoided when the next iteration only uses new
facts derived by the previous iteration.
   
In the context of the P2 system~\cite{Loo-condie-garofalakis-p2}, which was
created for developing declarative networking programs, the previous strategy is
not suitable since it is centralized, therefore a new strategy called
\emph{Pipelined Semi-Na\"{\i}ve}~(PSN) evaluation was developed for distributed
computation. PSN evaluation evaluates one fact at the time by firing any rule
that is derivable using the new fact and older facts. If the new fact is already
in the database, then the fact is not used for derivation since it would derive
redudant facts.

 LM's evaluation strategy is similar to PSN, however, it considers the whole
 database of facts when firing rules due to the existence of rule priorities.
 For instance, when a node fires a rule, new facts added for the local node are
 considered as a whole in order to select the next inference rule. In the case
 of PSN, each new fact would be considered separately. Still, PSN and the LM
 strategy are both asynchronous strategies which take advantage of the nature of
 distributed and parallel architectures, respectively. Finally, an important
 distinction that should be made between PSN and LM is that PSN is for logic
 programs with only persistent facts, which results in deterministic results,
 however because LM uses linear facts, it follows a \emph{don't care} or
 \emph{committed choice} non-determinism, which may result in different results
 depending on the order of computations.

\paragraph{CHR implementations} Many basic optimizations used in the LM compiler
such as join optimizations and the use of different data structures for indexing
facts were inspired in work done on CHR~\cite{DBLP:journals/corr/cs-PL-0408025}.
Wuille et al.~\cite{42866} have described a CHR to C compiler that follows some
of the ideas presented in this chapter and De Koninck et al.~\cite{chrp} showed
how to compile CHR programs with dynamic priorities into Prolog. The novelty of
our work focuses on supporting a combination of comprehensions, aggregates and
rule priorities.

