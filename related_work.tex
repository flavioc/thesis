We next give a brief introduction to the key concepts of declarative
programming, graph-based programming models, linear logic, coordination and
provability.

\section{Declarative Programming}

Many programming models have been developed in order to make parallel programs
both easier to write and reason about. The most popular examples of such
paradigms are \emph{logic programming} and \emph{functional programming}.  In
logic languages such as Prolog, researchers took advantage of the
non-determinism of proof-search to evaluate subgoals in parallel with models
such as \emph{or-parallelism} and
\emph{and-parallelism}~\cite{Gupta:2001:PEP:504083.504085}.  In functional
languages, the stateless nature of computation allows multiple expressions to
evaluate safely in parallel.  This has been initially explored in several
languages, such as NESL~\cite{Blelloch:1996:PPA:227234.227246} or
Id~\cite{Nikhil93anoverview}, and later implemented in more modern languages
such as Haskell~\cite{Chakravarty07dataparallel}.

Recently, there has been an increasing interest in declarative and data-centric
languages.  MapReduce~\cite{Dean:2008:MSD:1327452.1327492}, for instance, is a
popular data-centric programming model that is optimized for large clusters. The
scheduling and data sharing model is very simple: in the \emph{map phase}, data
is transformed at each node and the result reduced to a final result in the
\emph{reduce phase}.

Another declarative approach that is regaining popularity is
Datalog~\cite{Ullman:1990:PDK:533142}, a bottom-up logic programming language
that was the inspiration for the original Meld.  Traditionally used in deductive
databases, it is now being increasingly used in different fields such as
distributed networking~\cite{Loo-condie-garofalakis-p2}, sensor
nets~\cite{Chu:2007:DID:1322263.1322281} and cloud computing~\cite{alvaro:boom}.

\section{Graph-Based Programming Models}

Like the original Meld, many programming systems also model the application as a
graph where computation will be performed.  Some good examples are the Dryad,
Pregel, GraphLab, Ligra, Grace and Galois systems.

The Dryad system~\cite{Isard:2007:DDD:1272996.1273005} combines computational
vertices with communication channels (edges) to form a data-flow graph. The
program is scheduled to run on multiple processors or cores and data is
partitioned during runtime. Routines that run on computational vertices are
sequential, with no synchronization.

The Pregel system~\cite{Malewicz:2010:PSL:1807167.1807184} is also graph based,
although programs have a more strict structure. They must be represented as
a sequence of iterations where each iteration is composed of computation and
message passing.  Pregel is specially suited to work on big graphs and to
scale to large architectures.

GraphLab~\cite{GraphLab2010} is a C++ framework for developing parallel machine
learning algorithms. While Pregel uses message passing, GraphLab allows nodes to
have read/write access to different scopes through different concurrent access
models in order to balance performance and data consistency. While some programs
only need to access the local node's data, others may need to update edge
information. Each consistency model will provide different guarantees that are
better adapted to some algorithms. GraphLab also provides different schedulers
that dictate the order in which node's are computed.

Ligra~\cite{Shun:2013:LLG:2517327.2442530} is lightweight framework for large scale
graph processing on a single multicore machine. Ligra exploits the fact that
most huge graph datasets available today can be made to fit in the main memory
of commodity servers. Ligra is a simple framework that exposes 
two main interfaces: \texttt{EdgeMap} and \texttt{VertexMap}. The former applies
a function to a subset of edges of the graph, while the latter applys a function
to a subset of vertices. The functions passed as arguments are applied to either
a single edge or a single vertex and the user must ensure that the function can
be executed in parallel. The framework allows the use of \emph{Compare-and-Swap}
instructions when implementing functions in order to avoid race conditions.

Grace~\cite{wang:asynchronous} is another graph-based framework for multicore
machines. Unlike Ligra, Grace programs are implemented from implemented from the
point of view of a vertex. Each vertex and edge can be customized with different
data depending on the target application. By default, programs are executed
iteratively: for each iteration the vertex program reads incoming messages
located in the edges, performs computation and sends messages to the outbound
edges. Since iterative programs require synchronization after each iteration,
Grace allows the user to relax these constraints and implement customizable
execution policies by implementing code for describing which vertices are
allowed to run and in which order. The order is dictated by assigning a
\emph{scheduling priority value}.

Galois~\cite{Pingali:2011:TPA:1993316.1993501} is a parallel programming model
with irregular applications based on graphs, trees and sets. A Galois parallel
algorithm is viewed as a parallel application of an \emph{operator} over an
irregular data structure which generate \emph{activies} on the data structure.
Such operator may, for instance, be applied on the
node of the graph in order to change its data or change the structure of its
neighborhood, allowing for data structure changes. In Galois, we consider
\emph{active elements}, which are nodes of the graph where computation needs to
be performed, the \emph{neighborhood} containing the nodes of the graph required
to perform an operator, and \emph{ordering} which dictactes the order in which
operators are applied. From the point of view of the programer, the active
elements are represented in a worklist, while operators can be implemented on
top of iterators of the worklist. Galois supports speculative execution by
allowing operator rollback when an operator requires the use of a node that is
held by another operator.

\subsection{Languages for Sensor Networks}

Sensor nets are another interesting domain for programming languages that deal
with graph-like networks.  Programming languages such as
Hood~\cite{Whitehouse:2004:HNA:990064.990079},
Tinydb~\cite{Madden:2005:TAQ:1061318.1061322} or
Regiment~\cite{Newton:2007:RMS:1236360.1236422} have been proposed for this
particular domain, providing support for data collection and aggregation over
the network.  These systems assume that the network remains static and nodes
stay in place.

Other languages such as Pleiades~\cite{Kothari:2007:REP:1250734.1250757},
LDP~\cite{4543691} or Proto~\cite{Beal:2006:IEE:1137236.1137354} go beyond
static networks and support dynamic reconfiguration. In Pleiades, the
programmer writes an application from the point of view of the whole
sensor network and the compiler transforms it into code that can be run on
each individual node.  LDP is a language derived from a method for
distributed debugging, that allows it to efficiently detect conditions on
variably-sized groups of nodes. It is based on the tick model, generating
a new set of condition matchers throughout the ensemble on each tick.
Like Pleiades and LDP, Proto also compiles global programs into locally
executed code.

\section{Linear Logic}

Linear logic is a substructural logic proposed by Jean-Yves
Girard~\cite{Girard95logic:its} that extends intuitionistic logic with the
concept of \emph{truth as resources}. Instead of seeing the truth as immutable,
truth is now something that can be consumed during the proof process.

Since computer science is focused on processes and algorithms, linear logic has
been used in many areas of computing such as programming languages, game
semantics, concurrent programming, knowledge representation, etc.  Due to the
resource interpretation of the logic, linear logic presents a good basis for
programming languages with a structured way to manage state.

In the context of the Curry-Howard correspondence~\cite{howard:tfatnoc}, linear
logic has been applied in programming languages as a mechanism to implement
\emph{linear types}. Linear types or also sometimes known as \emph{uniqueness
types} are types that force objects to be used exactly once. Surprisingly,
such types add mutable state to functional languages because they enforce a
linear view of state, allowing the language to naturally support concurrency,
input/output and data structure's updates.  Arguably, the most popular
language that features uniqueness types is the Clean programming
language~\cite{JFP:1349748}.  Monads~\cite{Wadler:1997:DI:262009.262011},
made popular with the Haskell programming language, are another interesting
way to add state to functional languages. While monads tend to be more
powerful than linear types, they also ensure equational reasoning in the
presence of mutable data structures and I/O effects.

As we will see in the next chapters of this proposal, linear logic programming
is a different approach than either monads or linear types.  While the latter
are mechanisms that enhance functional programming with state, the former uses
state as a foundation, since the program's database is both the state and the
program, because it drives the computation forward through rule application.
However, our new language can also interact with the outside world through
sensing and action facts, which are special facts that return information about
the world and act on the outside world, respectively.

\section{Coordination}

In this section, we explore programming languages and programming models that
allow coordination and/or scheduling of computation and/or processing units.

\subsection{Programming Languages}

Many programming languages follow what is called the coordination
paradigm~\cite{Papadopoulos98coordinationmodels}. This form of distributed
programming divides execution in two parts: \emph{computation}, where the actual
computation is performed, and \emph{coordination}, which deals with
communication and cooperation between processing units. This paradigm attempts
to clearly distinguish between these two parts by providing abstractions for
coordination in an attempt to provide architecture and system-independent forms
of communication.

We can identify two main types of coordination models:

\begin{description}
   \item[Data-Driven:]
   
   In a data-driven model, the state of the computation depends on both the data
   being received or transmitted by the processes and the current configuration
   of the coordinated processes. The coordinated process is not only responsible
   for reading and manipulating the data but is also responsible for
   coordinating itself and/or other processes. Each process must intermix the
   coordination directives provided by the coordination model with the
   computation code. While these directives have a very clear interface, it is
   in the programmer's responsibility to use them correctly.

   \item[Task-Driven:]
   
   In this model, the coordination code is more cleanly separated from the
   computation code. While in data-driven models, the content of the data
   exchanged by the processes will affect how the processes coordinate with each
   other, in a task-driven model, the process behavior depends only on the
   coordination patterns that are setup before hand. This means that the
   computation component is defined as a black box and there are clearly defined
   interfaces for input/output. These interfaces are usually defined as a
   full-fledged coordination language and not as simple directives present in
   the data-driven models.  \end{description}

Linda~\cite{linda} is probably the most famous coordination model. Linda
implements a data-driven coordination model and features a \emph{tuple space}
that can be manipulated using the following coordination directives:
\texttt{out(t)} writes a tuple \texttt{t} into the tuple space; \texttt{in(t)}
reads a tuple using the template \texttt{t}; \texttt{rd(t)} retrieves a copy of
the tuple \texttt{t} from the tuple space; and \texttt{eval(p)} puts a process
\texttt{p} in the tuple space and executes it in parallel.  Linda processes do
not need to know the identity of other processes because processes only
communicate through the tuple space.  Linda can be implemented on top of many
popular languages by simply creating a communication and storage mechanism for
the tuple space and then adding the directives as a language library.

The original Meld~\cite{ashley-rollman-iclp09} can also be seen as a kind of
data-driven coordination language. The important distinction is that in Meld
there's no explicit coordination directives. When Meld rules are activated they
may derive facts that need to be sent to a neighboring robot. In turn, this will
activate computation on the neighbor. Robot communication is implemented by
\emph{localizing} the program rules and then by creating \emph{communication
rules}.

The proposed LM language also implements communication rules, however it goes
further because some facts, action facts, can change how the processing units
schedule nodes to be executed, namely, which node is to be computed next, which
may in turn change the program's final result. This result in a more complete
inter-play between coordination code and data.

\subsection{Programming Models}

The Galois~\cite{Pingali:2011:TPA:1993316.1993501} programming model implements
autonomous scheduling by default, where activities may be rolled back in case of
conflict. However, it is possible to employ a concrete scheduling strategy for
coordinating parallel execution in order to improve execution and avoid
conflicts.  First, there is \emph{compile-time coordination}, where the
scheduling ordered is computed during compilation and is pre-defined before the
program is executed. Secondly, there is \emph{runtime coordination}, where the
order of activities is computed during execution. The execution of the algorithm
proceeds in rounds: first, a set of non-conflicting activities is computed and
then executed by applying the operator; conflicting activities are postponed to
the next round. The third and last scheduling strategy is \emph{just-in-time
coordination} where the order of activities is defined by the underlying data
structure where the operator is applied (for instance, computing on a graph
may depend on its topology).

In the context of the Galois model, Nguyen et al.~\cite{nguyen11} expanded the
concept of runtime coordination with the introduction of a flexible approach to specify
scheduling policies for Galois programs. This approach was motivated by the fact
that some algorithms can be executed faster if computations use better
scheduling strategies. The scheduling language specifies 3 main scheduler types:
\texttt{FIFO} (First-In First-Out), \texttt{LIFO} (Last-In First-Out) and
\texttt{OrderedByMetric} (order activities by some metric). These schedulers can
be composed and synthesized without requiring users to write complex concurrent
code.

Elixir~\cite{Prountzos:2012:ESS:2384616.2384644} is a domain specific language
that builds on top of the Galois and allows easy specification of scheduling
strategies.  The main idea behind Elixir is that the user should be able to
specify how operator application is scheduled and the framework will compile
this high level specification to low level code using the provided scheduling
specification. One of the motivating examples is the Single Source Shortest Path
program that can be specified using multiple scheduling specifications,
generating different well-known shortest path algorithms such as the
Dijkstra or Bellman-Ford algorithm. Unlike the work of Nguyen et
all.~\cite{nguyen11}, Elixir does not allow graph mutations.

Halide~\cite{Ragan-Kelley:2013:HLC:2491956.2462176} is a language and compiler
for image processing pipelines with the goal of optimizing parallelism, locality
and re-computation. Halide decouples the algorithm definition from its execution
strategy, allowing the compiler to find which execution strategy may be the best
for optimizing for locality and parallelism. The language allows the programmer
to specify the scheduling strategy, allowing the programmer to decide the order
of computations, what intermediate results need to be stored, how to split the
data among processing units and how to use vectorization and the well-known
sliding window mechanism. However, the compiler is able to use stochastic search
to find good schedules for Halide pipelines. Notably, experimental results
indicate that automatic search sometimes leads to better execution than
hand-written code.

\section{Provability}

Many techniques and formal systems have been devised to help reason about
parallel programs.  One such example is the
Owicki-Gries~\cite{Owicki:1976:VPP:360051.360224} deductive system for proving
properties about imperative parallel programs (deadlock detection, termination,
etc).  It extends Hoare logic with a stronger set axioms such as parallel
execution, critical section and auxiliary variables. The formal system can be
successfully used in small imperative programs, although using it on languages
such as C is difficult since they do not restrict the use of shared variables.

Some formal systems do not build on top of a known programming paradigm, but
instead create an entirely new formal system for describing concurrent systems.
Process calculus such as $\pi$-calculus~\cite{Milner:1999:CMS:329902} is a good
example of this.  The $\pi$-calculus describes the interactions between
processes through the use of channels for communication. Interestingly, channels
can also be transmitted as messages, allowing for changes in the network of
processes.  Given two processes, $\pi$-calculus is able to prove that they
behave the same through the use of bi-simulation equivalence.

Another interesting model is Mobile UNITY~\cite{Roman97anintroduction}. The
basic UNITY~\cite{UNITY} model assumes that statements could be executed
non-deterministically in order to create parallelism. This principle is applied
to prove properties about the system.  Mobile UNITY transforms UNITY by adding
locations to processes and removing the nondeterministic aspect from local
processes. Processes could then communicate or move between locations.

Since the original Meld is based on logic programming, it has been used to
produce proofs of correctness.  Meld program code is amenable to mechanized
analysis via theorem checkers such as Twelf~\cite{twelf}, a logic system
designed for analyzing program logics and logic program implementations.  For
instance, a meta-module based shape planner program was proven to be
correct~\cite{dewey-iros08,ashley-rollman-iclp09} under the assumption that
actions derived by the program are always successfully applied in the outside
world.  While the fault tolerance aspect is lax, the planner will always reach
the target shape in finite time.  The sketch of the proof is presented in Dewey
et al.~\cite{dewey-iros08}
