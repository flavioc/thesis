Most software applications are developed using an \emph{imperative model}, where
there is a sequence of steps that the processor must do and a \emph{memory area}
where the processor stores and retrieves data during the course of execution.
With the increase in clock frequency on traditional single-core processors for
the last few decades, applications could run faster without any changes in the
source code. Today, processor manufactures are now focusing on increasing the
number of cores per processor due to the stalling of clock frequencies. This
exposes the disadvantages of the imperative model since it does not take
advantage of multiple processing cores. Note that sequential models are also
unsuitable in applications that need to be run in a cluster of computers because
the processors in the cluster do not share the same memory area and require
communication to coordinate computation.

Different programming models have been suggested to solve the limitations of the
imperative model and allow programmers to exploit parallelism. We classify these
models into three main classes: \emph{automatic parallelism}, \emph{declarative models}, and
\emph{parallel programming}.

In automatic parallelism, we have a compiler that transforms sequential programs
into parallel code. A clear example of automatic parallelism is the
\emph{instruction level parallelism}, which is available on computer
architectures to allow processors to execute multiple instructions at the same
time. On the software side, the compiler should be able to re-order instructions
of the sequential program to allow the hardware to take full advantage of
parallelism. Still, this particular solution has a limited application because
important parallelization opportunities are available at a higher level of
abstraction.

The second proposed solution include declarative models, which are declarative
programming models that completely abandon the sequential model and introduce
new programming models that are easier to parallelize. We dwelve on these topics
on the next section.

Finally, we have \emph{parallel programming} where sequential applications are
modified using new concurrency or communication constructs that allow the
programmer to explicitly exploit parallelism. It is required that the programmer
efficiently code to split the computation among the processing units and
coordinate the computation between them.

Exploiting parallelism using parallel programming is known to be hard since it
is difficult to coordinate processing units effectively and without bugs. The
very nature of parallel execution means increased non-determinism during
execution, which leads to many possible code interleavings that the programmer
needs to be aware of. Many well-known sequential algorithms are not trivially
parallelizable and require complete new approaches to be scalable. Moreover,
non-determinism of execution makes it hard to prove properties of the program
because the simpler assumptions of the sequential model no longer hold.

\subsection{Main Programming Models}

Parallelism has been traditionally classified into two classes: \emph{data
parallelism} and \emph{task parallelism}. In data parallelism, the data is
partitioned among the processing units and each unit performs the same
computation on their piece of data. In task parallelism, the program is split
into different tasks that are then assigned to processing units. If the data or
tasks are well-defined and regular, where they take the same time to be
completed, then parallelization is trivial. However, issues arise when it is
hard to partition the tasks or the tasks that need to be completed are not
static but are dynamically generated during execution. Furthermore, some tasks
may dependencies on other tasks and a correct execution ordering is required. In
such situations, the programmer is required to implement a \emph{scheduler} that
efficiently assigns tasks to processing units and is able to \emph{balance} the
load among those units. A scheduler may use a \emph{centralized strategy} where
there is a \emph{master} processing unit that makes the decision about
distributing work or the scheduler uses a \emph{distributed strategy} where each
processing unit is able to perform \emph{work stealing}~\cite{Blumofe:1999} on
other units for more work.

We now describe the main parallel programming models available for writing
parallel programs.

\paragraph{Shared Memory and Threads}

We mentioned before that the sequential model uses a memory area to store and
retrieve data. The \emph{shared memory model}~\cite{Mellor-Crummey:1991} makes
use of this area to allow communication between \emph{threads}, which are
processing units that have their own execution flow but share the same memory
area. The existence of a shared memory area makes it easier to share data
between threads, however the access to data accessed by multiple threads needs
to be protected, otherwise the data may be become inconsistent. Many constructs
are available to ensure \emph{mutual exclusion} such as
\emph{locks}~\cite{Silberschatz:2008}, \emph{semaphores}~\cite{Dijkstra:2002},
\emph{mutexes}~\cite{Silberschatz:2008}, and \emph{condition
variables}~\cite{Hoare:1974}.

\paragraph{Message Passing}

In the \emph{message passing} model, processing units do not share the same
memory area. Instead, processing units send messages to each other to coordinate
parallel execution. Message passing is well suited for programming clusters of
computers, where it is not possible to have a shared memory area, however
message processing take more time than reading from a shared memory area.  The
most well known framework for message passing is Message Passing
Interface~(MPI~)~\cite{Forum:1994}.

Message passing is also used as foundation to implement higher level parallel
programming models such as \emph{Remote Procedure
Call}~(RPC)~\cite{Birrell:1984}, which allows a program to call a procedure that
is seamlessly executed on another machine, removing the need of explicit
parallel code.

