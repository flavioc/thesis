\begin{comment}
Parallelism has been traditionally classified into two classes: \emph{data
parallelism} and \emph{task parallelism}. In data parallelism, the data is
partitioned among the processing units and each unit performs the same
computation on their piece of data. In task parallelism, the program is split
into different tasks that are then assigned to processing units. If the data or
tasks are well-defined, relatively independent and regular (i.e., they take the
same amount of time to be completed) then parallelization can be trivial.
However, issues arise when it is hard to partition the tasks or the tasks that
need to be completed are not static but are dynamically generated during
execution. To complicate matters even further, some tasks may depend on other
tasks being completed in order to be started. In such situations, the programmer
is required to implement a \emph{scheduler} that efficiently assigns tasks to
processing units and is able to \emph{balance} the load among those units. A
scheduler may use a \emph{centralized strategy} where there is a \emph{master}
processing unit that makes work distribution decisions or the scheduler uses a
\emph{distributed strategy} where each processing unit is able to perform
\emph{work stealing} or \emph{work sharing}~\cite{Blumofe:1999} on other units
to improve load balance.
\end{comment}

A popular paradigm for implementing parallelism is \emph{explicit parallel
programming}, where parallelism must be explicitly defined and all the
mechanisms for coordinating parallel execution must be specified by the
programmer. Explicit parallel programming is made possible through the use of
imperative programming extended with either a multi-threaded or message passing
model of programming.

\begin{description}

   \item[Multi-Threaded Model] In multi-threaded parallelism, the programmer
      must coordinate the execution of multiple threads of control by sharing
      state through a shared memory area. The existence of a shared memory area
      makes it easy to share data between workers, however, access to data from
      multiple workers needs to be protected, otherwise it might become
      inconsistent. Many constructs are available to ensure \emph{mutual
      exclusion} such as \emph{locks}~\cite{Silberschatz:2008},
      \emph{semaphores}~\cite{Dijkstra:2002},
      \emph{mutexes}~\cite{Silberschatz:2008}, and \emph{condition
      variables}~\cite{Hoare:1974}.

   \item[Message Passing] For message passing, communication is not done through a
      shared memory area, but by allowing, as the name says, messages to be sent
      between threads of control or processes. Message passing is well suited for
      programming clusters of computers, where it is not possible to have a shared
      memory area, however message processing is more costly than shared memory
      area due to the extra work required to send and serialize messages.  The most
      well known framework for message passing is the Message Passing
      Interface~(MPI~)~\cite{Forum:1994}.

\end{description}


Since explicit parallel programming is hard to get right due to the
non-deterministic nature of parallel execution, other, less explicit, parallel
programming models were devised in order to aliviate some of the problems
inherent to the model. One such problem is the \emph{fork/join} model which has
made popular with the Cilk~\cite{Bluemofe:1995:CEM:209936.209958} programming
language. In Cilk, the programmer writes C programs that can spawn parallel
tasks and then explicitly join (or wait) until the spawned tasks are complete.
An advantage of Cilk over more pure explicit parallel programming is that the
parallel control is performed by the compiler and runtime system, allowing for
automatic control over the threads of control through the use of techniques such
as \emph{work stealing}, where spawned tasks can be stolen by other processors.
However, the programmer still must explicitly indicate where procedures can be
spawned in parallel and where they should join.

Another model that uses the fork/join model is the
X10~\cite{Charles:2005:XOA:1094811.1094852} programming language where spawned
procedures are called activities. However, X10 innovates by introducing the
concept of places, which are processes that run on different machines and do not
share any memory area. This is also commonly called the \emph{partitioned global
address space}~(PGAS) model, since the memory area is partitioned among
processes in order to increase locality. While X10 solves many of the
communication and synchronization issues between threads of control and
processes, it is still the job of the programmer to effectively use the parallel
programming constructs in order to take advantage of parallelism.
