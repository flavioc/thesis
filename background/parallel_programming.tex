Most software applications are developed using an \emph{imperative model} where
there is a sequence of steps that the processor must do and a \emph{memory area}
where the processor stores and retrieves data during the course of execution.
The increase of clock frequency on single-core processors during the last
decades, has allowed applications using the imperative model to run faster
without any changes in the source code. Today, processor manufactures are now
focusing on increasing the number of cores per processor due to the stalling of
clock frequencies. This exposes the disadvantages of the imperative model since
it does not take advantage of multiple processing cores. Imperative models are
also unsuitable for distributed applications that need to be run in a cluster of
computers because the processors in the cluster do not share the same memory
area and require communication to coordinate computation.

Different programming models have been suggested to solve the limitations of the
imperative model and allow programmers to exploit parallelism. We classify these
models into three main classes: \emph{automatic parallelism}, \emph{declarative models}, and
\emph{parallel programming}.

In automatic parallelism, we have a compiler that transforms imperative programs
into parallel code. A clear example of automatic parallelism is the
\emph{instruction level parallelism}, which is available on computer
architectures to allow processors to execute multiple instructions at the same
time. On the software side, the compiler re-orders instructions of the
imperative program to allow the hardware to take full advantage of parallelism.
Still, this particular solution has limited applicability because important
parallelization opportunities are available at a different, higher, level of abstraction.

The second proposed solution includes declarative models, which are declarative
programming models that completely abandon the imperative model and introduce
new programming models that are easier to parallelize. We dwelve on these topics
on the next section.

Finally, we have \emph{parallel programming} where imperative applications are
modified using new concurrency or communication constructs that allow the
programmer to explicitly exploit parallelism. It is required that the programmer
writes code to efficiently split computation among processing units and allow
sharing of data between processing units.

Exploiting parallelism using parallel programming is known to be hard since it
is difficult to coordinate processing units effectively and without bugs. The
very nature of parallel execution means increased non-determinism during
execution which leads to many possible code interleavings that the programmer
needs to be aware of. Furthermore, many well-known imperative algorithms are not
trivially parallelizable and require complete new approaches to run in a
scalable fashion. Moreover, non-determinism of execution makes it hard to prove
properties of the program because the simpler assumptions of the imperative
model no longer hold under the new programming model.

\subsection{Main Programming Models}

Parallelism has been traditionally classified into two classes: \emph{data
parallelism} and \emph{task parallelism}. In data parallelism, the data is
partitioned among the processing units and each unit performs the same
computation on their piece of data. In task parallelism, the program is split
into different tasks that are then assigned to processing units. If the data or
tasks are well-defined and regular (i.e., they take the same time to be
completed) then parallelization is trivial. However, issues arise when it is
hard to partition the tasks or the tasks that need to be completed are not
static but are dynamically generated during execution. To complicate matters even
further, some tasks may depend on other tasks being completed in order to be
started. In such situations, the programmer is required to implement a \emph{scheduler} that
efficiently assigns tasks to processing units and is able to \emph{balance} the
load among those units. A scheduler may use a \emph{centralized strategy} where
there is a \emph{master} processing unit that makes work distribution decisions
or the scheduler uses a \emph{distributed strategy} where each
processing unit is able to perform \emph{work stealing}~\cite{Blumofe:1999} on
other units to improve load balance.

We now describe the main parallel programming models available for writing
parallel programs.

\paragraph{Shared Memory and Threads}

We mentioned before that the imperative model uses a memory area to store and
retrieve data. The \emph{shared memory model}~\cite{Mellor-Crummey:1991} makes
use of this area to allow communication between \emph{threads}, which are
processing units that have their own execution flow but share the same memory
area. The existence of a shared memory area makes it easier to share data
between threads, however, the access to data accessed by multiple threads needs
to be protected, otherwise the data may become inconsistent. Many constructs
are available to ensure \emph{mutual exclusion} such as
\emph{locks}~\cite{Silberschatz:2008}, \emph{semaphores}~\cite{Dijkstra:2002},
\emph{mutexes}~\cite{Silberschatz:2008}, and \emph{condition
variables}~\cite{Hoare:1974}.

\paragraph{Message Passing}

In the \emph{message passing} model, processing units do not share the same
memory area. Instead, processing units send messages to each other to coordinate
parallel execution. Message passing is well suited for programming clusters of
computers, where it is not possible to have a shared memory area, however
message processing is more costly than shared memory area due to extra work
required to send and serialize messages.  The most well known framework for
message passing is the Message Passing Interface~(MPI~)~\cite{Forum:1994}.

Message passing is also used as foundation to implement higher level parallel
programming models such as \emph{Remote Procedure
Call}~(RPC)~\cite{Birrell:1984}, which makes it possible to seamlessly call a
procedure which is executed remotely, removing the need for explicit message
passing.

