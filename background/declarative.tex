Another form of parallel programming is implicit parallel programming, a style
of programming commonly present in declarative programming languages.  The
declarative style of programming allows the programmer to concentrate more on
what the problem is, rather than telling the computer the steps it needs to do
to solve the problem. In turn, this gives freedom to the language implementation
to work out the details of parallelism, solving many of the issues that arise
when writing explicit parallel programs. The programmer writes code without
having to deal with parallel programming constructs and the compiler
automatically parallelizes the program in order to take advantage of multiple
threads of execution. Unfortunately, this comes at a cost of low programmer
control over how computation is performed, which may not be optimal under every
circumstance. However, declarative programming remains attractive because formal
reasoning is performed at a higher level and does not have to deal with low
level parallel programming constructs such as locks or message passing.

\subsection{Functional Programming}

Functional programming languages are considered declarative programming
languages and are based on the \emph{lambda calculus}, a model for function
application and abstraction.

In pure functional programming languages, the side effect free nature of
computation allows multiple expressions to evaluate safely in
parallel~\cite{Loidl:2003}. This has been realized in in languages such as
Id~\cite{Nikhil93anoverview} and SISAL~\cite{gaudiot2001sisal} with relative
success. However, this kind of parallelism still remains elusive in the
functional programming community since practical functional programs have, in
general, a high number of fine-grained parallel tasks, which makes it harder for
a compiler to schedule computations efficiently~\cite{haskell_tutorial}.
Alternative approaches such as \emph{data
parallelism}~\cite{Blelloch:1996:PPA:227234.227246,Chakravarty07dataparallel,peytonjones:2008},
\emph{semi-explicit parallelism}~\cite{Marlow:2010,Fluet:2010}, and
\emph{explicit parallelism}~\cite{harris2005composable} have shown to be more
effective in practice.

Data parallelism attempts to partition data among a set of processing units and
then apply the same operation on the data. The simplest form of data parallelism
is \emph{flat data parallelism}, where the data is flat (list or array) and is
easily partitioned. However, functional applications are composed of many recursive
data manipulation operations, which is not a good fit for flat data parallelism.
However, in the NESL language~\cite{Blelloch:1996:PPA:227234.227246}, a new compiler
transformation was proposed that could take a program using \emph{nested data
parallelism} and turn it into a program using flat data parallelism, which is
easier to parallelize. This approach has been later implemented in more
modern languages such as Haskell~\cite{Chakravarty07dataparallel}. The main
advantage of this approach is that it remains true to the original goal of
implicit parallelism.

In semi-explicit parallelism, the programmer uses an API to tell the runtime
system which computations should be carried out in parallel, thus avoiding the
fine-grain granularity problem. An example of such API are the so-called called
\emph{sparked computations}~\cite{Marlow:2010}, which have been made available
in the Haskell programming language and express the possibility of performing
speculative computations which are going to be needed in the future. In a sense,
sparked computations can be seen as \emph{lazy futures}~\cite{Baker:1977}. This
type of construct was also realized in the Manticore language~\cite{Fluet:2010},
which introduces the \texttt{pval} binding form for creating potential parallel
computations. Interestingly, Manticore also combines support for parallel arrays
in the style of NESL and explicit parallelism in the form of a \texttt{spawn}
keyword that allows the programmer to create threads that communicate using
message passing.

\subsection{Logic Programming}

Another example of declarative programming that is suitable for implicit
parallel programming is logic programming.
Logic programming is usually based on classical logics, such as the
\emph{predicate and propositional calculus}, or in non-classical logics such as
\emph{constructive} or \emph{intuitionistic} logic, where the goal is to
construct different models of logical truth.

Logical systems define their own meaning of truth and a set of consistent rules
to manipulate truth. Proofs about statements in a logical system are then
constructed by using the rules appropriately. For instance, the system of
\emph{propositional logic} contains the \emph{modus ponens} rule, where truth is
manipulated as follows: if $P$ implies $Q$ and $P$ is known to be true,
then $Q$ must also be true. For example, if we know that ``it is raining'' and that ``if
it is raining then the grass is wet'', we can prove that ``the grass is wet'' by
using the \emph{modus ponens} rule.

Logic programming arises when the proof search strategy in a logical system is
fixed. In the case of propositional logic, the following programming model can
be devised:

\begin{itemize}
   \item A set $R$ of implications of the form ``$P$ implies $Q$'' represents the
      program;
   \item A set $D$ of truths of the form ``$P$'' represents the database of
      facts;
   \item Computation proceeds by proof search where implications in $R$ are used
      to derive new truths using facts from $D$;
   \item Every new truth generated using \emph{modus ponens} is added back to
      $D$.
\end{itemize}

This particular proof search mechanism is called \emph{forward-chaining} (or
\emph{bottom-up}) \emph{logic programming}, since it starts from the initial
facts and then uses inference rules (\emph{modus ponens}) to derive new truth.
The proof search may then stop if the search has found a particular proposition
to be true or a state of \emph{quiescence} is reached (it is not possible to
derive any more truths).

An alternative proof search strategy is \emph{backward-chaining} (or
\emph{top-down}) \emph{logic programming}. In this style of programming, we want
to know if a particular proposition is true and then work backwards using the
inference rules. For instance, consider the implications: (i) ``if it is raining
then the grass is wet'' and (ii) ``if the weather forecast for today is rain then
it is raining'' and the proposition (iii) ``the weather forecast for today is
rain''. If we want to know if (iv) ``the grass is wet'', we select the
implication (i) and attempt to prove ``it is raining'' since it is required by
(i) to prove (iv). Next, the goal proposition becomes ``it
is raining'' and the conclusion of implication (ii) matches and thus we have to
prove ``the weather forecast for today is rain'', which can be proved
immediately using proposition (iii).

Prolog~\cite{Colmerauer:1993:BP:154766.155362} was one of the first logic
programming languages to become available, yet it still one of the most popular
logic programming languages in use today. Prolog is based on \emph{First Order
Logic}, a logical system that extends propositional logic with predicates and
variables. Prolog is a backward-chaining logic programming language where a
program is composed of a set of rules of the form ``$P$ implies $Q$'' that can be
activated by inputing a query.  Given a query $q(\hat{x})$, a Prolog interpreter
will work backwards by matching $q(\hat{x})$ against the head $Q$ of a rule. If
found, the interpreter will then try to match the body $P$ of the rule,
recursively, until it finds the program base facts. If the search procedure
succeeds, the interpreter finds a valid substitution for the $\hat{x}$ variables
in the given query.

Datalog~\cite{Ramakrishnan93asurvey,Ullman:1990:PDK:533142} is a
forward-chaining logic programming language originally designed for deductive
databases. Datalog has been traditionally used in deductive databases, but is
now being increasingly used in other fields such as sensor
nets~\cite{Chu:2007:DID:1322263.1322281}, cloud computing~\cite{alvaro:boom}, or
social networks~\cite{Seo:2013:DSD:2556549.2556572}.  In Datalog, the program is
composed of a database of facts and a set of rules.  Datalog programs first
populate the database with the starting facts and then saturate the database
using rule inference. In Datalog, logical facts are persistent and once a fact
is derived, it cannot be deleted. However, there has been a growing interest in
integrating linear logic~\cite{girard-87} into bottom-up logic programming,
allowing for both fact assertion and
retraction~\cite{Chang03ajudgmental,Lopez:2005:MCL:1069774.1069778,simmons-lla,cruz-iclp14},
which is one of the topics of this thesis.

In logic programming languages such as Prolog, researchers took advantage of the
non-determinism of proof-search to evaluate subgoals in parallel. The most
famous models are \emph{OR-parallelism} and
\emph{AND-parallelism}~\cite{Gupta:2001:PEP:504083.504085}. When performing
proof search with two implications of the form ``$P$ implies $Q$'' and ``$R$
implies $Q$'' then we have OR-parallelism because the proof search can select
``$P$ implies $Q$'' and try to prove $P$ but also select ``$R$ implies $Q$'' and
try to prove $R$. In AND-parallelism, there is an implication of the form ``$P$
and $R$ implies $Q$'' and, to prove $Q$, it is possible to prove $P$ and $Q$ at
the same time. AND-parallelism becomes more complicated when $P$ and $Q$
actually depend on each other, for example, if $P = \mathtt{prop}_1(\hat{x})$
and $R = \mathtt{prop}_2(\hat{x}, \hat{y})$ then the set variables $\hat{x}$
must be the same in the two propositions. This issue does not arise in
OR-parallelism, however AND-parallelism may be better when the program is more
deterministic (i.e., it contains less alternative implications) since rule's
bodies have more subgoals than the heads.

In Datalog programs, parallelism arises naturally because new logical facts may
activate multiple inference rules and thus generate more
facts~\cite{Ganguly:1990:FPP:93597.98724,Seib:1991:PDP:113413.113435,Wolfson:1988:DPL:971701.50242}.
A trivial parallelization can be done by splitting the rules among processing
units, however this may require sharing of logical facts depending on the rule
partitioning~\cite{Wolfson:1988:DPL:971701.50242}. Another alternative is to
partition the logical facts among the
machines~\cite{183073,Loo-condie-garofalakis-p2}, where rules are restricted in
order to facilitate fact partitioning and communication. The LM language
presented in this thesis follows this latter approach.

\subsubsection{Origins of LM}

LM is a direct descendant of Meld by Ashley-Rollman et
al.~\cite{ashley-rollman-iclp09,ashley-rollman-derosa-iros07wksp}, a logic
programming language developed in the context of the Claytronics
project~\cite{goldstein-computer05}. Meld is a language suited for programming
massively dynamic distributed systems made of modular robots. While mutable
state is not supported by Meld, Meld performs \emph{state management} on the
persistent facts by keeping a consistent database of facts whenever there is a
change in the starting facts. If an axiom is no longer true, everything derived
from that axiom is retracted. Likewise, when a fact becomes true, the database
is immediately updated to take the new logical fact into account. To take
advantage of these state management facilities, Meld supports \emph{sensing} and
\emph{action} facts. Sensing facts are derived from the state of the world
(e.g., temperature, new neighbor node) and action facts have an
effect on the world (e.g., movement, light emission).

Meld was inspired by the P2 system~\cite{Loo-condie-garofalakis-p2}, which
includes a logic programming language called NDlog~\cite{Loo:EECS-2006-177} for
writing network algorithms declaratively. Many ideas about state management were
already present in NDlog.  NDlog is essentially a Datalog based language with a
few extensions for declarative networking.
\subsection{Data-Centric Languages}

Recently, there has been increasing interest in declarative data-centric
languages. MapReduce~\cite{Dean:2008:MSD:1327452.1327492}, for instance, is a
popular data-centric programming model that is optimized for large clusters. The
scheduling and data sharing model is simple: in the \emph{map phase}, data
is transformed at each node and the result reduced to a final result in the
\emph{reduce phase}. In order to facilitate the writing of programs over large
datasets, SQL-like languages such as
PigLatin~\cite{Olston:2008:PLN:1376616.1376726} have been developed. PigLatin
builds on top of MapReduce and allows the programmer to write complex data-flow
graphs, raising the abstraction and ease of programmability of MapReduce
programs. An alternative to PigLatin/MapReduce is
Dryad~\cite{Isard:2007:DDD:1272996.1273005} that allows programmers to design
arbitrary computation patterns using DAG abstractions. It combines computational
vertices with communication channels (edges) that are automatically scheduled to
run on multiple computers/cores.

LM, the language presented in this thesis, also follows the data-centric
approach since it is designed for writing programs over graph data structures.
In Section~\ref{section:language:related}, we discuss data-centric languages in
more detail and present some other programming systems for data-centric parallel
computation and how they compare to LM.

\subsection{Granularity Problem}

In order for implicit parallelism to be effective in practice, its
implementation must know which computations should be done in parallel and which
computations are better performed sequentially due to the overhead of task
synchronization. This is commonly known as the \emph{granularity problem} and it
is a crucial issue for good parallel performance. If the implementation uses
coarse grained tasks, then this may lead to poor load balancing and thus poor
scalability since the tasks take too long. On the other hand, if fine grained
tasks are used, then scalability will suffer due to the overhead of
synchronization.

In functional programming, every expression can potentially be computed in
parallel. If such parallelism is explored eagerly, it will rapidly lead to
an enormous overhead due to the amount of fine grained parallelism.  The same
problem arises in logic programming~\cite{logprog_granularity} where multiple
subgoals can be proven in parallel at each point in the program, generating
large proof trees. It is then fundamental that declarative programming languages
target the right level of granularity in order to achieve good scalability and
performance.

Several solutions have been proposed and implemented to tackle this granularity
problem. As seen in the previous sections, some languages provide explicit
keywords that allow the programmer to give hints about which parts of the
program can be parallelized. A more general solution has been devised by Acar et
al.~\cite{Acar:2011} with the Parallel Algorithm Scheduling Library~(PASL), a
library that attempts to provide a general solution to task scheduling and
granularity control by providing a combination of user-provided asymptotic cost
functions and execution time profiling. PASL comes with a novel work stealing
algorithm~\cite{acar-chargueraud-rainey-13-sched} based on private deques that
further reduces the overhead of task creation and task sharing. LM also solves
the granularity problem but by modeling the application as a graph of nodes
where nodes can compute independently of other nodes, allowing for nodes, which
represent tasks, to be grouped together into subgraphs to provide the right
level of granularity.

