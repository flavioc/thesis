Declarative programming languages are considered to be high-level programming
languages, because they allow the programmer to concentrate more on what the
problem is, rather than telling the computer the steps it needs to solve the
problem. Declarative programming has been hailed as a solution to 
parallel programming, since the problem of implementing the details of
parallelism is moved from the programmer to the compiler and runtime
environment. The programmer writes code without having to deal with parallel
programming constructs and the compiler automatically parallelizes the program
in order to take advantage of multiple threads of execution. Unfortunately, this
comes at a cost of low programmer control over how computation is performed,
which may not be optimal under every circumstance. However, declarative
programming remains attractive because formal reasoning is performed at a higher
level and does not have to deal with low level parallel programming constructs
such as locks or message passing.

\subsection{Functional Programming}

Functional programming languages are considered declarative programming
languages and are based on the \emph{lambda calculus}, a model for function
application and abstraction.

In pure functional programming languages, the side-effect free nature of
computation allows multiple expressions to evaluate safely in
parallel~\cite{Loidl:2003}. This so called \emph{implicit parallelism} has been
implemented in languages such as Id~\cite{Nikhil93anoverview} and
SISAL~\cite{gaudiot2001sisal} with relative success. However, this kind of
parallelism still remains elusive in the functional programming community since
practical functional programs have, in general, a high number of fine-grained
parallel tasks, which makes it harder for a compiler to schedule computations
efficiently~\cite{haskell_tutorial}.  Alternative approaches such as
\emph{semi-explicit parallelism}~\cite{Marlow:2010}, \emph{data
parallelism}~\cite{Blelloch:1996:PPA:227234.227246,Chakravarty07dataparallel,peytonjones:2008},
and \emph{explicit parallelism}~\cite{harris2005composable} have shown to be
more effective in practice.

In semi-explicit parallelism, the programmer uses an API to tell the runtime
system which computations should be carried out in parallel, thus avoiding the
fine-grain granularity problem. These kinds of annotations for coarse-grained
parallel computations, called \emph{sparked computations}~\cite{Marlow:2010},
have been made available in the Haskell programming language and express the
possibility of performing speculative computations which are going to be needed
in the future. In a sense, sparked computations can be seen as \emph{lazy
futures}~\cite{Baker:1977}.

Data parallelism attempts to partition data among a set of processing units and
then apply the same operation on the data. The simplest form of data parallelism
is \emph{flat data parallelism}, where the data is flat (list or array) and is
easily partitioned. However, functional applications are composed of many recursive
data manipulation operations, which is not a good fit for flat data parallelism.
However, in the NESL language~\cite{Blelloch:1996:PPA:227234.227246}, a new compiler
transformation was proposed that could take a program using \emph{nested data
parallelism} and turn it into a program using flat data parallelism, which is
easier to parallelize. This approach has been later implemented in more
modern languages such as Haskell~\cite{Chakravarty07dataparallel}. The main
advantage of this approach is that it remains true to the original goal of
implicit parallelism.

\subsection{Data-Centric Languages}

Recently, there has been increasing interest in declarative data-centric
languages. MapReduce~\cite{Dean:2008:MSD:1327452.1327492}, for instance, is a
popular data-centric programming model that is optimized for large clusters. The
scheduling and data sharing model is simple: in the \emph{map phase}, data
is transformed at each node and the result reduced to a final result in the
\emph{reduce phase}. In order to facilitate the writing of programs over large
datasets, SQL-like languages such as
PigLatin~\cite{Olston:2008:PLN:1376616.1376726} have been developed. PigLatin
builds on top of MapReduce and allows the programmer to write complex data-flow
graphs, raising the abstraction and ease of programmability of MapReduce
programs. An alternative to PigLatin/MapReduce is
Dryad~\cite{Isard:2007:DDD:1272996.1273005} that allows programmers to design
arbitrary computation patterns using DAG abstractions. It combines computational
vertices with communication channels (edges) that are automatically scheduled to
run on multiple computers/cores.

LM, the language presented in this thesis, also follows the data-centric
approach since it is designed for writing programs over graph data structures.
In Section~\ref{section:language:related}, we discuss data-centric languages in
more detail and present some other programming systems for data-centric parallel
computation and how they compare to LM.

\subsection{Logic Programming}

Another example of declarative programming is logic programming.
Logic programming is usually based on classical logics, such as the
\emph{predicate and propositional calculus}, or in non-classical logics such as
\emph{constructive} or \emph{intuitionistic} logic, where the goal is to
construct different models of logical truth.

Logical systems define their own meaning of truth and a set of consistent rules
to manipulate truth. Proofs about statements in a logical system are then
constructed by using the rules appropriately. For instance, the system of
\emph{propositional logic} contains the \emph{modus ponens} rule, where truth is
manipulated as follows: if $P$ implies $Q$ and $P$ is known to be true,
then $Q$ must also be true. For example, if we know that ``it is raining'' and that ``if
it is raining then the grass is wet'', we can prove that ``the grass is wet'' by
using the \emph{modus ponens} rule.

Logic programming arises when the proof search strategy in a logical system is
fixed. In the case of propositional logic, the following programming model can
be devised:

\begin{itemize}
   \item A set $R$ of implications of the form ``$P$ implies $Q$'' represents the
      program;
   \item A set $D$ of truths of the form ``$P$'' represents the database of
      facts;
   \item Computation proceeds by proof search where implications in $R$ are used
      to derive new truths using facts from $D$;
   \item Every new truth generated using \emph{modus ponens} is added back to
      $D$.
\end{itemize}

This particular proof search mechanism is called \emph{forward-chaining} (or
\emph{bottom-up}) \emph{logic programming}, since it starts from the initial
facts and then uses inference rules (\emph{modus ponens}) to derive new truth.
The proof search may then stop if the search has found a particular proposition
to be true or a state of \emph{quiescence} is reached (it is not possible to
derive any more truths).

An alternative proof search strategy is \emph{backward-chaining} (or
\emph{top-down}) \emph{logic programming}. In this style of programming, we want
to know if a particular proposition is true and then work backwards using the
inference rules. For instance, consider the implications: (i) ``if it is raining
then the grass is wet'' and (ii) ``if the weather forecast for today is rain then
it is raining'' and the proposition (iii) ``the weather forecast for today is
rain''. If we want to know if (iv) ``the grass is wet'', we select the
implication (i) and attempt to prove ``it is raining'' since it is required by
(i) to prove (iv). Next, the goal proposition becomes ``it
is raining'' and the conclusion of implication (ii) matches and thus we have to
prove ``the weather forecast for today is rain'', which can be proved
immediately using proposition (iii).

Prolog~\cite{Colmerauer:1993:BP:154766.155362} was one of the first logic
programming languages to become available, yet it still one of the most popular
logic programming languages in use today. Prolog is based on \emph{First Order
Logic}, a logical system that extends propositional logic with predicates and
variables. Prolog is a backward-chaining logic programming language where a
program is composed of a set of rules of the form ``$P$ implies $Q$'' that can be
activated by inputing a query.  Given a query $q(\hat{x})$, a Prolog interpreter
will work backwards by matching $q(\hat{x})$ against the head $Q$ of a rule. If
found, the interpreter will then try to match the body $P$ of the rule,
recursively, until it finds the program base facts. If the search procedure
succeeds, the interpreter finds a valid substitution for the $\hat{x}$ variables
in the given query.

Datalog~\cite{Ramakrishnan93asurvey,Ullman:1990:PDK:533142} is a
forward-chaining logic programming language originally designed for deductive
databases. Datalog has been traditionally used in deductive databases, but is
now being increasingly used in other fields such as sensor
nets~\cite{Chu:2007:DID:1322263.1322281}, cloud computing~\cite{alvaro:boom}, or
social networks~\cite{Seo:2013:DSD:2556549.2556572}.  In Datalog, the program is
composed of a database of facts and a set of rules.  Datalog programs first
populate the database with the starting facts and then saturate the database
using rule inference. In Datalog, logical facts are persistent and once a fact
is derived, it cannot be deleted. However, there has been a growing interest in
integrating linear logic~\cite{girard-87} into bottom-up logic programming,
allowing for both fact assertion and
retraction~\cite{Chang03ajudgmental,Lopez:2005:MCL:1069774.1069778,simmons-lla,cruz-iclp14},
which is one of the topics of this thesis.

In logic programming languages such as Prolog, researchers took advantage of the
non-determinism of proof-search to evaluate subgoals in parallel. The most
famous models are \emph{OR-parallelism} and
\emph{AND-parallelism}~\cite{Gupta:2001:PEP:504083.504085}. When performing
proof search with two implications of the form ``$P$ implies $Q$'' and ``$R$
implies $Q$'' then we have OR-parallelism because the proof search can select
``$P$ implies $Q$'' and try to prove $P$ but also select ``$R$ implies $Q$'' and
try to prove $R$. In AND-parallelism, there is an implication of the form ``$P$
and $R$ implies $Q$'' and, to prove $Q$, it is possible to prove $P$ and $Q$ at
the same time. AND-parallelism becomes more complicated when $P$ and $Q$
actually depend on each other, for example, if $P = \mathtt{prop}_1(\hat{x})$
and $R = \mathtt{prop}_2(\hat{x}, \hat{y})$ then the set variables $\hat{x}$
must be the same in the two propositions. This issue does not arise in
OR-parallelism, however AND-parallelism may be better when the program is more
deterministic (i.e., it contains less alternative implications) since rule's
bodies have more subgoals than the heads.

In Datalog programs, parallelism arises naturally because new logical facts may
activate multiple inference rules and thus generate more
facts~\cite{Ganguly:1990:FPP:93597.98724,Seib:1991:PDP:113413.113435,Wolfson:1988:DPL:971701.50242}.
A trivial parallelization can be done by splitting the rules among processing
units, however this may require sharing of logical facts depending on the rule
partitioning~\cite{Wolfson:1988:DPL:971701.50242}. Another alternative is to
partition the logical facts among the
machines~\cite{183073,Loo-condie-garofalakis-p2}, where rules are restricted in
order to facilitate fact partitioning and communication. The LM language
presented in this thesis follows this latter approach.

\subsection{Origins of LM}

LM is a direct descendant of Meld by Ashley-Rollman et
al.~\cite{ashley-rollman-iclp09,ashley-rollman-derosa-iros07wksp}, a logic
programming language developed in the context of the Claytronics
project~\cite{goldstein-computer05}. Meld is a language suited for programming
massively dynamic distributed systems made of modular robots. While mutable
state is not supported by Meld, Meld performs \emph{state management} on the
persistent facts by keeping a consistent database of facts whenever there is a
change in the starting facts. If an axiom is no longer true, everything derived
from that axiom is retracted. Likewise, when a fact becomes true, the database
is immediately updated to take the new logical fact into account. To take
advantage of these state management facilities, Meld supports \emph{sensing} and
\emph{action} facts. Sensing facts are facts derived from the state of the world
(e.g., temperature, new neighbor node) and action facts are facts that have an
effect on the world (e.g., move), changing the underlying sensing facts.

Meld was inspired by the P2 system~\cite{Loo-condie-garofalakis-p2}, which
includes a logic programming language called NDlog~\cite{Loo:EECS-2006-177} for
writing network algorithms declaratively. Many ideas about state management were
already present in NDlog.  NDlog is essentially a Datalog based language with a
few extensions for declarative networking.
