
In this section, we explore programming languages and programming models that
allow coordination and/or scheduling of computation and/or processing units.

\subsection{Programming Languages}

Many programming languages follow what is called the coordination
paradigm~\cite{Papadopoulos98coordinationmodels}. This form of distributed
programming divides execution in two parts: \emph{computation}, where the actual
computation is performed, and \emph{coordination}, which deals with
communication and cooperation between processing units. This paradigm attempts
to clearly distinguish between these two parts by providing abstractions for
coordination in an attempt to provide architecture and system-independent forms
of communication.

We can identify two main types of coordination models:

\begin{description}
   \item[Data-Driven:]
   
   In a data-driven model, the state of the computation depends on both the data
   being received or transmitted by the processes and the current configuration
   of the coordinated processes. The coordinated process is not only responsible
   for reading and manipulating the data but is also responsible for
   coordinating itself and/or other processes. Each process must intermix the
   coordination directives provided by the coordination model with the
   computation code. While these directives have a very clear interface, it is
   in the programmer's responsibility to use them correctly.

   \item[Task-Driven:]
   
   In this model, the coordination code is more cleanly separated from the
   computation code. While in data-driven models, the content of the data
   exchanged by the processes will affect how the processes coordinate with each
   other, in a task-driven model, the process behavior depends only on the
   coordination patterns that are setup before hand. This means that the
   computation component is defined as a black box and there are clearly defined
   interfaces for input/output. These interfaces are usually defined as a
   full-fledged coordination language and not as simple directives present in
   the data-driven models.  \end{description}

Linda~\cite{linda} is probably the most famous coordination model. Linda
implements a data-driven coordination model and features a \emph{tuple space}
that can be manipulated using the following coordination directives:
\texttt{out(t)} writes a tuple \texttt{t} into the tuple space; \texttt{in(t)}
reads a tuple using the template \texttt{t}; \texttt{rd(t)} retrieves a copy of
the tuple \texttt{t} from the tuple space; and \texttt{eval(p)} puts a process
\texttt{p} in the tuple space and executes it in parallel.  Linda processes do
not need to know the identity of other processes because processes only
communicate through the tuple space.  Linda can be implemented on top of many
popular languages by simply creating a communication and storage mechanism for
the tuple space and then adding the directives as a language library.

Another early coordination language is Delirium~\cite{Delirium}. Unlike Linda,
which is embedded into another language, Delirium actually embeds operators
written in other languages inside the Delirium language. The advantages of
Delirium are improved abstraction and easier debugging because sequential
operators are isolated from the coordination language.

Linda and Delirium are limited in the sense that the programmer can only
coordinate the scheduling of processing units, while placement of data is left
to the implementation. LM differs from those languages because coordination acts
on data instead of processing units. The abstraction is then raised by
considering data and algorithmic aspects of the program instead of focusing on
how processing units are used. Furthermore, LM is both a coordination language
and a computation language and there is no distinction between the two
components.

The original Meld~\cite{ashley-rollman-iclp09} can also be seen as a kind of
data-driven coordination language. The important distinction is that in Meld
there's no explicit coordination directives. When Meld rules are activated they
may derive facts that need to be sent to a neighboring robot. In turn, this will
activate computation on the neighbor. Robot communication is implemented by
\emph{localizing} the program rules and then by creating \emph{communication
rules}.

The LM language also implements communication rules, however it goes further
because some facts, action facts, can change how the processing units schedule
nodes to be executed, namely, which node is to be computed next, which may in
turn change the program's final result. This result in a more complete
inter-play between coordination code and data.

\subsection{Programming Models}

The Galois~\cite{Pingali:2011:TPA:1993316.1993501} programming model implements
autonomous scheduling by default, where activities may be rolled back in case of
conflict. However, it is possible to employ a concrete scheduling strategy for
coordinating parallel execution in order to improve execution and avoid
conflicts.  First, there is \emph{compile-time coordination}, where the
scheduling ordered is computed during compilation and is pre-defined before the
program is executed. Secondly, there is \emph{runtime coordination}, where the
order of activities is computed during execution. The execution of the algorithm
proceeds in rounds: first, a set of non-conflicting activities is computed and
then executed by applying the operator; conflicting activities are postponed to
the next round. The third and last scheduling strategy is \emph{just-in-time
coordination} where the order of activities is defined by the underlying data
structure where the operator is applied (for instance, computing on a graph
may depend on its topology).

In the context of the Galois model, Nguyen et al.~\cite{nguyen11} expanded the
concept of runtime coordination with the introduction of a flexible approach to specify
scheduling policies for Galois programs. This approach was motivated by the fact
that some algorithms can be executed faster if computations use better
scheduling strategies. The scheduling language specifies 3 main scheduler types:
\texttt{FIFO} (First-In First-Out), \texttt{LIFO} (Last-In First-Out) and
\texttt{OrderedByMetric} (order activities by some metric). These schedulers can
be composed and synthesized without requiring users to write complex concurrent
code.

Elixir~\cite{Prountzos:2012:ESS:2384616.2384644} is a domain specific language
that builds on top of the Galois and allows easy specification of scheduling
strategies.  The main idea behind Elixir is that the user should be able to
specify how operator application is scheduled and the framework will compile
this high level specification to low level code using the provided scheduling
specification. One of the motivating examples is the Single Source Shortest Path
program that can be specified using multiple scheduling specifications,
generating different well-known shortest path algorithms such as the
Dijkstra or Bellman-Ford algorithm. Unlike the work of Nguyen et
all.~\cite{nguyen11}, Elixir does not allow graph mutations.

Halide~\cite{Ragan-Kelley:2013:HLC:2491956.2462176} is a language and compiler
for image processing pipelines with the goal of optimizing parallelism, locality
and re-computation. Halide decouples the algorithm definition from its execution
strategy, allowing the compiler to find which execution strategy may be the best
for optimizing for locality and parallelism. The language allows the programmer
to specify the scheduling strategy, allowing the programmer to decide the order
of computations, what intermediate results need to be stored, how to split the
data among processing units and how to use vectorization and the well-known
sliding window mechanism. However, the compiler is able to use stochastic search
to find good schedules for Halide pipelines. Notably, experimental results
indicate that automatic search sometimes leads to better execution than
hand-written code.

In contrast to the previous systems, LM stands alone in making coordination
(both scheduling and partitioning) a first-class programming construct and
semantically equivalent to computation. Furthermore, LM distinguishes itself by
supporting data-driven dynamic coordination, particularly for irregular data
structures. Elixir and Galois do not support coordination for data partitioning,
and, in Elixir, the coordination specification is separated from computation,
limiting the programmability of coordination. Compared to LM, Halide is
targeted for regular applications and therefore only supports compile time
coordination.
