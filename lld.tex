The Low Level Dynamic~(LLD) semantics remove all the non-deterministic choices
in the previous dynamics and makes them deterministic. The new semantics will do
the following:

\begin{itemize}

   \item Match rules by priority order;

   \item Determine the set of linear facts needed to match either the rule's LHS
      or the LHS of comprehensions/aggregates without guessing;

   \item Apply as many comprehensions as the database allows.

   \item Apply as many aggregates as the database allows.

\end{itemize}

While the implementation presented in Chapter~\ref{chapter:local} follows the
LLD semantics, there are several optimizations not implemented in LLD, such as:

\begin{itemize}
   \item Indexing: the implementation uses indexing for looking up facts using a
      specific argument;
   \item Better candidate rules: when selecting a rule to execute, the
      implementation filters out rules which do not have enough facts to be
      derived;
   \item Multiple rule derivation: the LLD semantics only execute one rule at
      the time, while the implementation is able to derive a rule multiple times
      when there are no conflicting rule;
   \item Matching and substitution: in the implementation, matching is done
      implicitly using variables and comparisons, while LLD uses the $\Psi$
      context to hold substitutions.
\end{itemize}

The complete set of inference rules for the LLD semantics are presented in
Appendix~\ref{sec:lld}.

LLD is specified as an \emph{abstract machine} and is represented as a sequence
of state transitions of the form $\trans{S_1}{S_2}$. HLD had many different
proof trees for a given triplet $\Gamma; \Delta; \Phi$ because HLD allows
choices to be made during the inference rules. For instance, in HLD any rule
could be selected to be executed. In LLD there is only one state sequence
possible for a given $\Gamma; \Delta; \Phi$ since there is no guessing involved.
LLD semantics present a complete step by step mechanism that is needed to
correctly evaluate a LM program. For instance, when LLD tries to apply a rule,
it will check if there are enough facts in the database and backtrack until a
rule can be applied.

\subsection{Application}

LLD shares exactly the same inputs and outputs as HLD. The inputs correspond to
the $\Gamma$ and $\Delta$ fact contexts and the list of rules $\Phi$, while the
outputs correspond to the newly asserted facts in $\Gamma'$ and $\Delta'$ and
the retracted facts which are put in the $\Xi'$ context.

The first difference between LLD and HLD start when picking a rule to derive.
Instead of guessing, LLD treats the list of rules as a stack and picks the first
rule $R_1$ to execute (the rule with the highest priority). The remaining rules
are stored as a \emph{continuation}. If $R_1$ cannot be matched because there
are not enough facts in the database, we backtrack and use the rule continuation
to pick the next rule and so on, until one rule can be successfully applied.

The machine starts with a database $(\Gamma; \Delta)$ and a list of rules
$\Phi$. The initial state is always $\dostate{\Delta}{\Phi}{\Gamma}{\Pi}$.
We start by picking the first rule $R_1$ from $\Phi$:

\input{lld/init}

If, after trying all the rules, there are no remaining candidate rules, the
machine enters into the $\m{next}$ state, which means that no more rules are
possible for this node and the machine should perform local computation on
another node.

\input{lld/fail}

In order to try a particular rule, we either need to unfold the $\forall$
connective, by adding its variable to the $\Psi$ context, or, initiate the matching
process when reaching the $\lolli$ connective. The variables in the $\Psi$
context, which are initially assigned to an unknown value $\_$, will later be
assigned to a concrete value as the matching process goes forward.

\input{lld/open_rule}
\input{lld/init_rule}

\subsection{Continuation Frames}

The most interesting aspects introduced by the LLD machine are the
\emph{continuation frame} and the \emph{continuation stack}. A continuation
frame acts as a choice point that is created during rule matching whenever we
try to match a fact expression against the database.  The frame considers all
the facts relevant to the expression given the current context $\Psi$.

The frame contains enough state to resume the matching process at the time of
its creation, therefore we can easily backtrack to the choice point and select
the next candidate fact from the database.  We keep the continuation frames in a
continuation stack for backtracking purposes. If, at some point there are no
candidate facts because the current variable assignments are not usable, we
update the top frame to try the next candidate fact. If all candidates are
exhausted, we pop the top frame and continue with the next available frame.

By using this match mechanism, we determine which facts need to be used to match
a rule.  Our LM implementation works like LLD, by iterating over the available
facts at each choice point and then committing to the rule if the matching
process succeeds. However, while the implementation only attempts to match rules
when the database has all the facts required by the rule's LHS, LLD is more
na\"{i}ve in this aspect because it tries all rules in order.


\subsection{Structure of Continuation Frames}

We have two continuation frame types, depending on the type of the candidate
facts.\footnote{All continuation frames have an implicit $\Psi$ context that
models variable assignments, including variable names, values and their
locations in the terms. This is important if we want to model variable
assignments and matchings.}

\subsubsection{Linear Continuation Frames}

There are two types of continuation frames. Linear frames use the form
$\lframe{\Delta}{\Delta''}{p(\widehat{x})}{\Omega; \Psi}{\Delta'}{\Omega'}$, where:

\begin{description}

   \item[$p(\widehat{x})$] atomic proposition that created this
      frame. The predicate for the proposition is $p$;

   \item[$\Delta$] multi-set of linear facts that are not of predicate $p$ plus
      all the other candidate facts of the predicate $p$ we have already
      tried, including a fact $p$, which is the current candidate fact;

   \item[$\Delta''$] facts of predicate $p$ that match $p(\widehat{x})$ which we
      haven't tried yet. It is a multi-set of linear facts;


   \item[$\Omega$] ordered list of remaining terms needed to match;

   \item[$\Delta'$] multi-set of linear facts we have consumed to reach this point;

   \item[$\Omega'$] terms matched already using $\Delta'$ and $\Gamma$;
   \item[$\Psi$] dictionary of variable assignments (includes variable and
      value).
\end{description}

\subsubsection{Persistent Continuation Frame}

Persistent frames are slightly different since they only need to keep track of
remaining persistent candidates. They are structured as
$\pframe{\Gamma''}{\Delta}{\bang
   p(\widehat{x})}{\Omega; \Psi}{\Delta'}{\Omega'}$:

\begin{description}
   \item[$\bang p(\widehat{x})$] persistent atomic proposition that created
      this frame;
   \item[$\Gamma''$] remaining candidate facts that match $\bang p(\widehat{x})$;
   \item[$\Delta$] multi-set of linear facts not consumed yet;

   \item[$\Omega$] ordered list of terms needed to match past this
   frame;

   \item[$\Delta'$] multi-set of linear facts consumed up-to this frame;
   \item[$\Omega'$] terms matched up-to this point using $\Delta'$ and $\Gamma$;
   \item[$\Psi$] dictionary of variable assignments (includes variable and value).
\end{description}


\subsection{Match}\label{sec:lld_body_match}

The matching state for the LLD machine uses the continuation stack to try
different combinations of facts until a match is achieved.  The state is
structured as $\matstate{A \lolli
   B}{\rulestk}{\lstack{C}}{\Gamma}{\Delta}{\Omega}{\Delta' \rightarrow
      \Omega'}$, where:

\begin{description}
   \item[$A \lolli B$] rule being matched: $A$ is the rule's LHS and $B$ the RHS;

   \item[$\rulestk$] rule continuation to be used if the current rule fails.
   Contains the original $\Delta_N$ and the rest of the rules $\Phi$;

   \item[$\lstack{C}$] ordered list of frames representing the continuation
   stack used for matching $A$;

   \item[$\Delta$] multi-set of linear facts still available to complete the
   matching process;

   \item[$\Omega$] ordered list of deconstructed RHS terms to match;

   \item[$\Delta'$] multi-set of linear facts from the original $\Delta_N$ that
   were already consumed ($\Delta', \Delta = \Delta_N$);

   \item[$\Omega'$] parts of $A$ already matched. They are in the form $P_1
   \otimes \dotsb \otimes P_n$. The idea is to use term equivalence and the fact
   that $\feq{\Omega, \Omega'}{A}$ to justify $\mz{\Gamma}{\Delta'}{A}$ when the
   matching process completes.

\end{description}

Not shown in the matching state is the context $\Psi$ that maps variables to
values. At the start of matching, the $\widehat{x}$ variables are set as
\emph{undefined}. Matching then uses facts from $\Delta$ and $\Gamma$ to match
the terms of the rule's LHS represented as $\Omega$. During the process
continuation frames are pushed into $\lstack{C}$ and if the matching process
fails, we use $\lstack{C}$ to restore the process using different candidate
facts. New facts also update the variables in the $\Psi$ context by assigning
them concrete values.

\subsubsection{Linear fact expression}

The first 2 state transitions are used when the head of $\Omega$ is a linear fact
expression $p$.

In the first transition we find $p_1$ and $\Delta''$ as facts from the database
that match $p$'s hidden and partially initialized arguments.  Context $\Delta''$
is stored in the second argument of the new continuation frame but is passed
along with $\Delta$ since the facts have not been consumed yet (just $p_1$).

The second transition deals with the case where there are no candiate facts and
thus a different machine state is used for enabling backtracking.

Note that the proposition $p_1, \Delta'' \prec p$ indicates that facts
$\Delta'', p_1$ satisfy the constraints of $p$ while $\Delta \npreceq p$
indicates that no fact in $\Delta$ satisfies $p$. Both propositions use the
omitted variable context $\Psi$ in order to replace the variables of $p$.

\input{lld/match-p}

\subsubsection{Persistent fact expressions}

The next 2 state transitions are used when the head of $\Omega$ contains a
persistent fact expression $\bang p$. They are identical to the previous 2
transitions but they deal with the persistent context $\Gamma$.

\input{lld/match-bang-p}

\subsubsection{Other expressions}

Finally, we have the transitions that deconstruct the other LHS terms and a
final transition to initiate the RHS derivation.

\input{lld/match-other}

\subsection{Backtracking}\label{sec:lld_match_cont}

The backtracking state of the machine reads the top of the continuation stack
$\lstack{C}$ and restores the matching process with a different candidate fact
from the continuation frame. The state is written as $\contstate{A \lolli
B}{\rulestk}{\lstack{C}}{\Gamma}$, where:

\begin{description}
   \item[$A \lolli B$] the rule being matched;
   \item[$\rulestk$] next available rules if the current rule does not match;
   the rule continuation;
   \item[$\lstack{C}$] the continuation stack for matching $A$;
\end{description}

\subsubsection{Linear continuation frames}

The next two state transitions handle linear continuation frames on the top of the
continuation stack. The first transition selects the next candidate fact $p_1$ from the
second argument of the linear frame and updates the frame. Otherwise, if we have
no more candidate facts, we pop the continuation frame and backtrack to the
remaining continuation stack.

\input{lld/cont-p}

\subsubsection{Persistent continuation frames}

We also have the same two kinds of inference rules for persistent continuation
frames.

\input{lld/cont-bang-p}

\subsubsection{Empty continuation stack}

Finally, if the continuation stack is empty, we simply force execution to try
the next inference rule in $\Phi$.

\input{lld/cont-empty}

\subsection{Derivation}

Once the list of terms $\Omega$ of the LHS is exhausted, we derive the rule's
RHS. The derivation state simply iterates over $B$, the rule's RHS, and derives
terms into the corresponding new contexts. The state is represented as
$\derstatex{\Gamma}{\Delta}{\Xi}{\Gamma_1}{\Delta_1}{\Omega}$ with the following
meaning:

\begin{enumerate}
   \item[$\Gamma$] set of persistent facts;

   \item[$\Delta$] multi-set of remaining liner facts;

   \item[$\Xi$] multi-set of linear facts consumed up-to this point;

   \item[$\Gamma_1$] set of persistent facts derived;

   \item[$\Delta_1$] multi-set of linear facts derived;

   \item[$\Omega$] remaining terms to derive as an ordered list. We start with
   $B$ if the original rule is $A \lolli B$.

\end{enumerate}

\subsubsection{Fact templates}

When deriving either $p$ or $\bang p$ we have the following two inference rules:

\input{lld/der-p}

\subsubsection{RHS deconstruction}

The following two inference rules deconstruct the RHS list $\Omega$ from terms
created using either $\one$ or $\otimes$.

\input{lld/der-other}

\subsubsection{Aggregates}

We also have a transition for aggregates. The aggregate starts with a set of
values $\widehat{V}$ and an accumulator initialized as $\cdot$. The second state
initiates the matching process of the LHS $A$ of the aggregate (explained in
the next section).

\input{lld/der-agg}

\subsubsection{Successful rule}

Finally, if the ordered list $\Omega$ is exhausted, then the whole execution
process is done.  Note how the output arguments match the input arguments of the
$\done$judgment.

\input{lld/der-done}

\subsection{Aggregates}

\input{lld/aggregates}

