In Chapter~\ref{chapter:implementation}, we saw that an LM program operates on a graph data structure $G = (V, E)$ with nodes $V$ and edges $E$. Concurrent computation is performed non-deterministically by a set of execution threads $T$ that work on subsets of $G$.
Our implementation allows threads
to steal nodes from other threads in order to improve load balancing.  However,
there are many scheduling details that are left for our implementation to decide and which the programmer has no control over. How should a thread
schedule the computation of its sub-graph? Is node stealing beneficial to all
programs? What is the best sub-graph partitioning for a given LM program?  In order to allow custom node scheduling and partitioning, we introduce \emph{coordination facts}, a
mechanism that is indistinguishable from regular computation and that allows the
programmer to control how the runtime system executes programs. This is
an important first step in allowing the programmer to declaratively control execution. In Chapter~\ref{chapter:threads}, we further extend the language with thread-based facts to allow reasoning over the state of the execution threads.

\section{Motivation}\label{section:coord:rationale}

\input{coordination/rationale}

\section{Types of Facts}

\input{coordination/types}

\section{Scheduling Facts}\label{sec:coord:fifo}

\input{coordination/scheduling}

\section{Partitioning Facts}
\input{coordination/partitioning}

\section{Global Directives}\label{sec:coordination:global}

We also provide a few global coordination statements that cannot be specified
as sensing or action facts but are still important:

\input{coordination/global}
\section{Implementation}
\input{implementation/coord}

\section{Coordinating SSSP}
\input{coordination/coord_sssp}

\section{Applications}

\input{coordination/programs}

\section{Related Work}\label{sec:coordination:related}
\input{coordination/related_work}
\section{Chapter Summary}

In this chapter, we presented the set of coordination facts, a new declarative
mechanism for coordinating declarative parallel programs. Coordination facts are
implemented as sensing and action facts and allow the programmer to write
derivation rules that change how the runtime system schedules computation and
partitions the data in the parallel system, thus improving the executing time.
In terms of programming language design, our coordination mechanisms are unique
in the sense that they are treated like regular computation, which allows for
complex run-time coordination policies that are declarative and is part of the
main program's logic.
