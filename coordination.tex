In Chapter~\ref{chapter:implementation}, we saw that the set of nodes of an LM
program is represented as a graph data structure $G = (V, E)$ with nodes $V$ and
edges $E$ where $T$ threads perform work. Our implementation also allows threads
to steal nodes from other threads in order to improve load balancing.  However,
there are many scheduling details that are left undefined. How should a thread
schedule the computation of its sub-graph? Is node stealing beneficial to all
programs? What is the best sub-graph partitioning for a given LM program?  To
answer these questions, we introduce \emph{coordination facts}, a coordination
mechanism that is indistinguishable from regular computation and that allows the
programmer to specify custom scheduling and node partitioning policies. This is
an important first step in allowing the programmer to coordinate and improve
declarative programs.

\section{Motivation}\label{section:coord:rationale}

\input{coordination/rationale}

\section{Types of Facts}

\input{coordination/types}

\section{Scheduling Facts}\label{sec:coord:fifo}

\input{coordination/scheduling}

\section{Partitioning Facts}
\input{coordination/partitioning}

\section{Global Directives}\label{sec:coordination:global}

We also provide a few global coordination statements that cannot be specified
as sensing or action facts but are still important:

\input{coordination/global}
\section{Implementation}
\input{implementation/coord}

\section{Coordinating SSSP}
\input{coordination/coord_sssp}

\section{Applications}

\input{coordination/programs}

\section{Related Work}\label{sec:coordination:related}
\input{coordination/related_work}
\section{Chapter Summary}

In this chapter, we presented the set of coordination facts, a new declarative
mechanism for coordinating declarative parallel programs. Coordination facts are
implemented as sensing and action facts and allow the programmer to write
derivation rules that change how the runtime system schedules computation and
partitions the data in the parallel system, thus improving the executing time.
In terms of programming language design, our coordination mechanisms are unique
in the sense that they are treated like regular computation, which allows for
complex run-time coordination policies that are declarative and can be made part
of the main program's logic.
