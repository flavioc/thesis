In this section, we present solutions to well-known problems. We start with
straightforward graph-based problems such as bipartitness checking and two
versions of the PageRank program. Next, we present the LM version of the
QuickSort algorithm, which from a first impression may not fit well under the
programming paradigm offered by LM. Informal correctness and termination proofs
are also included to further show that such important properties are relatively
easy to prove for programs written in LM.

\subsection{Bipartiteness Checking}

The problem of checking if a graph is bipartite can be seen as a 2-color graph
coloring problem.  The code for this algorithm is shown in
Fig.~\ref{language:code:bichecking}. All nodes in the graph start as
\texttt{uncolored},
because they do not have a color yet. The axiom \texttt{visit(@1, 1)} is
instantiated at node \texttt{@1} (line 9) in order to color it with color 1.

If a node is \texttt{uncolored} and needs to be marked with a color \texttt{P}
then the rule in lines 11-12 is applied. We consume the \texttt{uncolored} fact
and derive a \texttt{colored(A, P)} to effectively color the node with
\texttt{P}. We also derive \texttt{visit(B, next(P))} in neighbor nodes to color
them with the other color. Line 

The coloring can fail if a node is already colored with a color \texttt{P} and
needs to be colored with a different color (line 15) or if it has already failed
(line 16).

\begin{figure}[h!]
\begin{Verbatim}[numbers=left,fontsize=\scriptsize]
type route edge(node, node).
type linear visit(node, int).
type linear uncolored(node).
type linear colored(node, int).
type linear fail(node).

fun next(int X) : int = if X <> 1 then 1 else 2 end.

visit(@1, 1).

visit(A, P), uncolored(A)
   -o {B | !edge(A, B) | visit(B, next(P))}, colored(A, P).

visit(A, P), colored(A, P) -o colored(A, P).
visit(A, P1), colored(A, P2), P1 <> P2 -o fail(A).
visit(A, P), fail(A) -o fail(A).
\end{Verbatim}
  \caption{Bipartiteness Checking program.}
  \label{language:code:bichecking}
\end{figure}

\subsubsection{Proof Of Correctness}

In order to show that the code in Fig.~\ref{language:code:bichecking} works as
intended, we first setup some invariants that hold throughout the execution of
the program. Assume that the set of nodes in the graph is represented as $N$.

\begin{invariant}[Node state]
Set of nodes $N$ is partitioned into 4 different states that represent the 4
possible states that a node can be in, namely:

\begin{itemize}
   \item $U$ (\texttt{uncolored} nodes)
   \item $F$ (\texttt{fail} nodes)
   \item $C_{true}$ (\texttt{colored(A, 1)} nodes)
   \item $C_{false}$ (\texttt{colored(A, 2)} nodes)
\end{itemize}
\end{invariant}
\begin{proof}
Initially, all nodes start in set $U$. All the 4 rules of the programs either
keep the node in the same set or exchange the node with another set.
\end{proof}

A bipartite graph is one where in every edge $a \leftrightarrow b$, there is a
valid assignment that makes $a$ member of set $C_{true}$ or $C_{false}$ and node
$b$ member of either $C_{false}$ or $C_{true}$ respectively.

\begin{lemma}[Bipartiteness
   Convergence]\label{language:lemma:bipartite_convergence}
   We now reason from the application of the program rules. After each
   application of an inference rule, one of the following will happen:

   \begin{itemize}
      \item Set $U$ will decrease and set $C_{true}$ or $C_{false}$ will
         increase, with a potential increase in the number of \texttt{visit}
         facts.
      \item Set $C_{true}$ or $C_{false}$ will stay the same, while the number
         of \texttt{visit} facts will be reduced.

      \item Set $C_{true}$ or $C_{false}$ will decrease and set $F$ will
         increase, while the number of \texttt{visit} facts will be reduced.

      \item Set $F$ will stay the same, while the number of \texttt{visit} facts
         decreases.
   \end{itemize}

\end{lemma}
\begin{proof}
Directly from the rules.
\end{proof}

From this invariant, it can be inferred that set $U$ never increases in size
and in a node transition from \texttt{uncolored} to \texttt{colored}, the
database may increase in size. For every other rule application, the database of
facts always decreases. This also means that the program will eventually
terminate, since it is limited by the number of \texttt{visit} facts that can be
generated.

\begin{theorem}[Bipartiteness Correctness]
If the graph is connected and bipartite then the nodes will be partitioned into
sets $C_{true}$ and $C_{false}$, while sets $F$ and $U$ are empty.
\end{theorem}
\begin{proof}
   By induction, we prove that uncolored nodes become part of either $C_{true}$
   and $C_{false}$ and, if there is an edge between nodes in the two sets then
   they have different colors.

   In the base case, we start with empty sets but node \texttt{@1} is made
   member of $C_{true}$. Rule 1 sends \texttt{visit} facts to the neighbors of
   \texttt{@1}, forcing them to be members of $C_{false}$.

   In the inductive case, we have sets $C'_{true}$ and $C'_{false}$ with some
   nodes already colored. From Lemma~\ref{language:lemma:bipartite_convergence},
   we know that $U$ always decreases. Since the graph is bipartite, events 3 and
   4 never happen since there is a possible partitioning of nodes. With event 1,
   we have set $C_{true} = C'_{true}, n$, (or $C_{false}$) where $n$ is the
   node and with event 2, the sets remain the same. Since the graph is
   connected, every node will be colored, therefore event 1 will happen for
   every node of the graph.
\end{proof}

\subsection{PageRank}

PageRank~\cite{Page:2001:MNR} is a well known graph algorithm that is used to
compute the relative relevance of web pages.  The code for a synchronous version
of the algorithm is shown in Fig.~\ref{code:pagerank}.  As the name indicates,
the pagerank is computed for a certain number of iterations. The initial
pagerank is the same for every page and is initialized in the first rule (lines
15-16) along with an accumulator.

The second rule of the program (lines 17-19) propagates a newly computed
pagerank value to all neighbors. The fact \texttt{neighbor-pagerank} informs the
neighbor node about the pagerank value of node \texttt{A} for iteration
\texttt{Iter + 1}. For every iteration, each node will accumulate the all the
\texttt{neighbor-pagerank} facts into the \texttt{accumulator} fact (lines
27-28). When all inbound neighbor pagerank values are accumulated, the third
rule (lines 21-25) is fired and a pagerank value is derived for iteration
\texttt{Iter}.

\begin{figure}[h!]
\begin{Verbatim}[numbers=left,fontsize=\scriptsize]
type outbound(node, node, float).
type linear pagerank(node, float, int).
type numInbound(node, int).
type linear accumulator(node, float Acc, int Left, int Iteration).
type linear neighbor-pagerank(node, node Neighbor, float Rank, int Iteration).
type linear start(node).

const damping = 0.85. // probability of user following a link in the current page.
const iterations = str2int(@arg1). // iterations to compute.
const pages = @world. // number of pages in the graph.

start(A).

start(A), !numInbound(A, T)
   -o accumulator(A, 0.0, T, 1), pagerank(A, 1.0 / float(pages), 0).

pagerank(A, V, Iter), // propagate new pagerank value
Iter < iterations
   -o {B, W | !outbound(A, B, W) | neighbor-pagerank(B, A, V * W, Iter + 1)}.

accumulator(A, Acc, 0, Iter), // new pagerank
!numInbound(A, T),
V = damping + (1.0 - damping) * Acc,
Iter <= iterations
   -o pagerank(A, V, Iter), accumulator(A, 0.0, T, Iter + 1).
	
neighbor-pagerank(A, B, V, Iter), accumulator(A, Acc, T, Iter)
   -o accumulator(A, Acc + V, T - 1, Iter).
\end{Verbatim}
\caption{Synchronous PageRank program.}
\label{language:code:pagerank}
\end{figure}

\subsubsection{Asynchronous PageRank}

We also have an asynchronous version of the algorithm that trades correctness
with convergence speed since it does not synchronize between iterations.
Figure~\ref{language:code:async_pagerank} shows the LM code for this particular
version, where two major differences can be observed: (1) there is a linear fact
\texttt{neighbor-pagerank} containing the most up-to-date pagerank value of a
neighbor node; (2) a new \texttt{update} fact that forces the node to re-compute
its pagerank by processing the currently available \texttt{neighbor-pagerank}
facts. Rules in lines 13-21 update the \texttt{neighbor-pagerank} values, while
rule in lines 23-29 asynchronously update the current pagerank value. This last
rule derives multiple \texttt{new-neighbor-rank} that is used to inform the
neighbor about the new pagerank value.

\begin{figure}[h!]
\begin{Verbatim}[numbers=left,fontsize=\scriptsize]
type outbound(node, node, float).
type linear pagerank(node, float, int).
type numInbound(node, int).
type linear neighbor-pagerank(node, node Neighbor, float Rank, int Iteration).
type linear new-neighbor-rank(node, node Neighbor, float Rank, int Iteration).
type linear update(A).
type linear sum-ranks(node, float).

pagerank(A, 1.0 / float(pages), 0).
update(A).
neighbor-pagerank(A, B, 1.0 / float(pages), 0). // pagerank of B is ...

// save incoming pagerank value.
new-neighbor-rank(A, B, New, Iteration),
neighbor-pagerank(A, B, Old, OldIteration),
Iteration > OldIteration
   -o neighbor-pagerank(A, B, New, Iteration).
new-neighbor-rank(A, B, New, Iteration),
neighbor-pagerank(A, B, Old, OldIteration),
Iteration <= OldIteration
   -o neighbor-pagerank(A, B, Old, OldIteration).

sum-ranks(A, Acc),
NewRank = damping/float(pages) + (1.0 - damping) * Acc,
pagerank(A, OldRank, Iteration)
      -o pagerank(A, NewRank, Iteration + 1),
         {B, W, Delta, Iter | !outbound(A, B, W), Delta = fabs(NewRank -
               OldRank) * W | new-neighbor-rank(B, A, NewRank, Iteration + 1),
               if Delta > bound then update(B) end}.

update(A), update(A) -o update(A).

update(A),
!numInbound(A, T)
   -o [sum => V | B, W, Val, Iter | neighbor-pagerank(A, B, Val, Iter)
         V = Val/float(T) | neighbor-pagerank(A, B, Val, Iter) | sum-ranks(A, V)].
\end{Verbatim}
\caption{Asynchronous PageRank program.}
\label{language:code:async_pagerank}
\end{figure}

\subsubsection{Proof Of Correctness}

To build the proof of correctness, we must again prove several program
invariants. This will help us prove that this partifcular program is similar to
a computation on a nonnegative matrix of of unit spectral radius, which has been
proven that it converges~\cite{DBLP:journals/corr/abs-cs-0606047,
Lubachevsky:1986:CAA:4904.4801}.

\begin{invariant}[Page Invariant]
Each page/node has a single \texttt{pagerank(A, Value, Iteration)} and:
\begin{itemize}
   \item For each outbound link, a single \texttt{\bang outbound(A, B, W)}.
   \item For each inbound link, a single \texttt{neighbor-pagerank(A, B, V, Iter)}.
   \item For each \texttt{\bang outbound(A, B, W)}, a \texttt{neighbor-pagerank(A,
      B, V, Iter}.
\end{itemize}
\end{invariant}

\begin{proof}

All axioms validate the 3 conditions of the variant. Note that the third
condition is also validated by the axioms, although not all axioms are shown in
the code.

In relation to rule application:

\begin{itemize}
   \item Rule 1: inbound link re-derived.
   \item Rule 2: inbound link re-derived.
   \item Rule 3: \texttt{pagerank/2} re-derived.
   \item Rule 4: Nothing happens.
   \item Rule 5: inbound links re-derived in the comprehension.
\end{itemize}
\end{proof}

\begin{lemma}[Neighbor rank lemma]
Given a fact \texttt{neighbor-pagerank(A, B, V, Iter)} and a set of facts
\texttt{new-neighbor-rank(A, B, New, Iter2)}, we end up with a single
\texttt{neighbor-pagerank(A, B, V', Iter')}, where \texttt{Iter} is the greater of
\texttt{Iter} or all of \texttt{Iter2'}.
\end{lemma}
\begin{proof}
By induction on the number of \texttt{new-neighbor-rank} facts.

Base case: \texttt{neighbor-pagerank} remains.

Inductive case: given one \texttt{new-neighbor-rank} fact:

\begin{itemize}
   \item Rule 1: the new iteration is older and thus \texttt{neighbor-pagerank}
   is replaced. By applying induction, we know that we will select either the
   new best iteration or a better iteration from the remaining set of
   \texttt{new-neighbor-rank} facts.
   \item Rule 2: the new iteration is not older and we keep the old
   \texttt{neighbor-pagerank} fact. By induction, we select the best from either
   the current iteration or some other (from the set).
\end{itemize}
\end{proof}

\begin{lemma}[Update lemma]
Given at least 1 \texttt{update/1} fact, rule 7 will run.
\end{lemma}
\begin{proof}
By induction on the number of \texttt{update} facts.

Base case: rule 5 will run.

Inductive case: rule 4 will run first because it has a higher priority, reducing
the number of \texttt{update} facts by one. By induction, we know that by
using the remaining \texttt{update} facts, rule 7 will run.
\end{proof}

\begin{lemma}[Pagerank update lemma]
(1) Given at least one \texttt{update} fact, the \texttt{pagerank(A, $V_{I}$,
I)} fact will be updated to become \texttt{pagerank(A, $V_{I + 1}$, I +
1)}, where \texttt{$V_{I + 1} = damping / P + (1.0 - damping)\sum_{B,
I} (W_{B} \times  N_{I,B})$}.

where $W_B = 1.0/T$ ($T$ from \texttt{\bang numInbound(A, $T$)})
and $N_{I,B}$ from \texttt{neighbor-pagerank(A, B, $N_{I, B}$, $I$)}.

(2) For all \texttt{B} outbound nodes (represented using \texttt{\bang outbound(A, B,
W)}, a \texttt{new-neighbor-rank(B, A, $V_{I+1}$, $I + 1$)} is generated.

(3) For all \texttt{B} outbound nodes (represented using \texttt{\bang outbound(A, B,
W)}), a \texttt{update(B)} is generated if 
$fabs(V_{I + 1} - V_{I}) \times W > bound$.
\end{lemma}
\begin{proof}
Using the Update lemma, rule 5 will necessarily run.

It derives \texttt{sum-ranks(A, $\sum_{B, I} (W_B \times N_{I,B})$)} and
fulfills (3).

\texttt{sum-ranks/2} will necessarily fire rule 6,
computing $V_{I+1}$ and updating \texttt{pagerank}. (2) and (3) are fulfilled
through the comprehension of rule 6.
\end{proof}

\begin{invariant}[New neighbor rank equality]
All \texttt{new-neighbor-rank(A, B, V, I)} facts are generated from a corresponding
\texttt{pagerank(B, V, I)} fact, therefore the iteration of any
\texttt{new-neighbor-rank} is at least the same or less than the iteration of
the current pagerank.
\end{invariant}
\begin{proof}
No axioms to prove.

\begin{itemize}
   \item Rule 3: true, new fact is generated.
   \item Rule 6: the fact is kept.
\end{itemize}
\end{proof}

\begin{invariant}[Neighbor rank equality]
All \texttt{neighbor-pagerank(A, B, V, I)} facts have one corresponding
\texttt{pagerank(B, V, I)} fact and the iteration of the \texttt{neighbor-pagerank}
is the same or less than the current iteration of the corresponding
\texttt{pagerank}.
\end{invariant}
\begin{proof}
By analyzing axioms and rules.

Axioms: true.

Rule cases:

\begin{itemize}
   \item Rule 1: uses \texttt{new-neighbor-rank} fact (use new neighbor rank
         equality invariant).
   \item Rule 2: same fact is re-derived.
\end{itemize}
\end{proof}

\begin{theorem}[Pagerank convergence]
The program will compute the pagerank of all nodes that is within \texttt{bound} error
of an asynchronous pagerank computation.
\end{theorem}
\begin{proof}

Using the program axioms, we start with the same pagerank value for all nodes.
The \texttt{\bang outbound(A, B, W)} fact forms a $n \times n$ square matrix (number
of nodes) and is the so-called "Google Matrix".  All the initial pagerank values
can be seen as a vector that adds up to $1$.

The pagerank computation from the "Pagerank update lemma" computes $V_{I + 1} =
damping / P + (1.0 - damping)\sum_{B, I'} (W_{B} \times N_{I',B})$, where $I' <=
I$
(from Neighbor rank equality invariant).

Consider that each node contains a column $G_i$ of the Google matrix. The
pagerank computation can then be represented as: \newline


$V_{I + 1} = G_i fix([N_{I_1, B_1}, ..., N_{I_p, B_p}])$ \hfill (1) \\


Where $p$ is the number of inbound links and $N_{I_j, B_j}$ is the value of
the \texttt{neighbor-pagerank(A, $B_j$, $N_{I_j, B_j}$, $I_j$)}. The $fix()$
function takes the neighbor vector and expands it with zeros corresponding to
nodes that are not inbound links.

From~\cite{DBLP:journals/corr/abs-cs-0606047, Lubachevsky:1986:CAA:4904.4801} we
know that equation (1) will improve (converge) the pagerank value, given that some new
neighbor pagerank values are sent to node $i$ and by the fact that $G_i$ is a
nonnegative matrix of unit spectral radius. Let's use induction by assuming that there
is at least one \texttt{update/1} fact that
schedules a node to improve its pagerank. We want to prove that such fact will
not only improve the node's pagerank but also the pagerank vector.
If the pagerank vector is now close enough (within \texttt{bound}), then the
program will terminate.

\begin{itemize}
   \item Base case: Since we have an \texttt{update} fact as an axiom, rule 7
   will compute a new pagerank (Pagerank update lemma) for all nodes that is
   improved (from equation (1)). From these updates, a new \texttt{update}
   fact is generated that correspond to nodes that have inbound links from the
   source node and need to update their pagerank. These \texttt{update} facts
   may not be generated if the pagerank vector is close enough to its real
   value.

   \item The induction hypothesis tells us that there is at least one node that
   has an \texttt{update} fact. From pagerank update lemma, this
   generates \texttt{new-neighbor-rank} facts if the new value differs
   significantly from the older value. When this happens and using the ``Neighbor
   rank lemma'', the target node will update its \texttt{neighbor-pagerank} fact
   with the newest iteration and then execute a valid pagerank computation that
   brings the pagerank vector close to its solution.

\end{itemize}

\end{proof}


\subsection{Quick-Sort}

The quick-sort algorithm is a divide and conquer sorting algorithm that works by splitting
a list of items into two sublists and then recursively sorting the two sublists.
To split the list, we pick a pivot element and put the items that are smaller than the pivot
into the first sublist and items greater than the pivot into the second list.

The quick-sort algorithm is interesting because it does not map immediately to the graph-based
model of LM. Our approach considers that the program starts with a single node where
the initial list is located. Then we split the list as usual and create two nodes
that will recursively sort the sublists. Interestingly, this will create a tree
that will look similar to a call tree in a functional language.

Fig.~\ref{code:quicksort} presents the code for the quick-sort algorithm in LM.
For each sublist to sort, we start with a \texttt{down} fact that must be (eventually)
transformed into an \texttt{up} fact, where the sublist in the \texttt{up} fact is sorted.
In line 11 we start with the initial list at node \texttt{@0}. Lines 13-16 will immediately
sort the list when the number of items is very small. Otherwise, we apply the rule in line 17.
\texttt{buildpivot} will first split the list using the pivot \texttt{X} using rules in
lines 23-26. When there is nothing more to split, we apply the rule in lines 19-21
that uses an exist construct to create nodes \texttt{B} and \texttt{C}. The sublists
are then sent to these nodes using \texttt{down} facts. Note, however, that we also
derive \texttt{back} facts, that will be used to send the sorted list back using the rule
in line 40.

When the sublists are finally sorted, we get two \texttt{sorted} facts that will match
against \texttt{waitpivot} in the rule located in lines 28-31. The sorted sublists
are appended and then an \texttt{up} fact is finally derived (line 37).

\begin{figure}[h!]
\small\begin{Verbatim}[numbers=left]
type route back(node, node).
type linear down(node, list int).
type linear up(node, list int).
type linear sorted(node, node, list int).
type linear buildpivot(node, list int, int, list int, list int).
type linear waitpivot(node, node, node, int).
type linear append(node, list int, list int).
type linear reverse(node, list int, list int, list int).
type linear reverse2(node, list int, list int).

down(@0, tosort).

down(A, []) -o up(A, []).
down(A, [X]) -o up(A, [X]).
down(A, [X, Y]), X < Y -o up(A, [X, Y]).
down(A, [X, Y]), X >= Y -o up(A, [Y, X]).
down(A, [X | L]) -o buildpivot(A, L, X, [], []).

buildpivot(A, [], X, Smaller, Greater)
   -o exists B, C. (back(B, A), down(B, Smaller),
            back(C, A), down(C, Greater), waitpivot(A, B, C, X)).

buildpivot(A, [Y | L], X, Smaller, Greater), Y <= X
   -o buildpivot(A, L, X, [Y | Smaller], Greater).
buildpivot(A, [Y | L], X, Smaller, Greater), Y > X
   -o buildpivot(A, L, X, Smaller, [Y | Greater]).
   
waitpivot(A, NodeSmaller, NodeGreater, Pivot),
sorted(A, NodeSmaller, Smaller),
sorted(A, NodeGreater, Greater)
   -o append(A, Smaller, [Pivot | Greater]).

append(A, L1, L2) -o reverse(A, L1, L2, []).

reverse(A, [], L2, L3) -o reverse2(A, L3, L2).
reverse(A, [X | L], L2, L3) -o reverse(A, L, L2, [X | L3]).
reverse2(A, [], Result) -o up(A, Result).
reverse2(A, [X | L1], L2) -o reverse2(A, L1, [X | L2]).

up(A, L), back(A, B) -o sorted(B, A, L).
\end{Verbatim}
  \caption{Quick-Sort program.}
  \label{code:quicksort}
\end{figure}
\normalsize


